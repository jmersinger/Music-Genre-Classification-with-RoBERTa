{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a72bd69-a821-4a47-af97-b1ad2170fc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Allocated GPU memory: 0.00 GB\n",
      "Reserved GPU memory: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Optionally allocate a fraction of GPU memory\n",
    "torch.cuda.set_per_process_memory_fraction(0.9, 0)\n",
    "\n",
    "# Check memory stats\n",
    "print(f'Allocated GPU memory: {torch.cuda.memory_allocated(device) / (1024 ** 3):.2f} GB')\n",
    "print(f'Reserved GPU memory: {torch.cuda.memory_reserved(device) / (1024 ** 3):.2f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "175073ea-4071-4aaf-a332-e354846b474c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.cleaning import download_and_clean\n",
    "\n",
    "artists, tracks = download_and_clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d54a8e77-2193-4fe4-8043-282cc51d056f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         id  followers                      genres  \\\n",
      "45   0VLMVnVbJyJ4oyZs2L3Yl2       71.0          ['carnaval cadiz']   \n",
      "46   0dt23bs4w8zx154C5xdVyl       63.0          ['carnaval cadiz']   \n",
      "47   0pGhoB99qpEJEsBQxgaskQ       64.0          ['carnaval cadiz']   \n",
      "48   3HDrX2OtSuXLW5dLR85uN3       53.0          ['carnaval cadiz']   \n",
      "136  22mLrN5fkppmuUPsHx6i2G       59.0  ['classical harp', 'harp']   \n",
      "\n",
      "                             name  popularity  \n",
      "45   Las Viudas De Los Bisabuelos           6  \n",
      "46              Los De Capuchinos           5  \n",
      "47             Los “Pofesionales”           7  \n",
      "48      Los Que No Paran De Rajar           6  \n",
      "136                   Vera Dulova           3  \n"
     ]
    }
   ],
   "source": [
    "print(artists.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07ae631a-3033-429d-8484-1f0d06c826a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       id                                name  popularity  \\\n",
      "2  07A5yehtSnoedViJAZkNnc  Vivo para Quererte - Remasterizado           0   \n",
      "3  08FmqUhxtyLTn6pAh6bk45       El Prisionero - Remasterizado           0   \n",
      "4  08y9GfoqCWfOGsKdwojr5e                 Lady of the Evening           0   \n",
      "5  0BRXJHRNGQ3W4v9frnSfhu                           Ave Maria           0   \n",
      "7  0IA0Hju8CAgYfV1hwhidBH                             La Java           0   \n",
      "\n",
      "   duration_ms  explicit              artists                  id_artists  \\\n",
      "2       181640         0  ['Ignacio Corsini']  ['5LiOoJbxVSAMkBS2fUm3X2']   \n",
      "3       176907         0  ['Ignacio Corsini']  ['5LiOoJbxVSAMkBS2fUm3X2']   \n",
      "4       163080         0      ['Dick Haymes']  ['3BiJGZsyX9sJchTqcSA7Su']   \n",
      "5       178933         0      ['Dick Haymes']  ['3BiJGZsyX9sJchTqcSA7Su']   \n",
      "7       161427         0      ['Mistinguett']  ['4AxgXfD7ISvJSTObqm4aIE']   \n",
      "\n",
      "  release_date  danceability  energy  ...  loudness  mode  speechiness  \\\n",
      "2   1922-03-21         0.434  0.1770  ...   -21.180     1       0.0512   \n",
      "3   1922-03-21         0.321  0.0946  ...   -27.961     1       0.0504   \n",
      "4         1922         0.402  0.1580  ...   -16.900     0       0.0390   \n",
      "5         1922         0.227  0.2610  ...   -12.343     1       0.0382   \n",
      "7         1922         0.563  0.1840  ...   -13.757     1       0.0512   \n",
      "\n",
      "   acousticness  instrumentalness  liveness  valence    tempo  time_signature  \\\n",
      "2         0.994          0.021800    0.2120   0.4570  130.418               5   \n",
      "3         0.995          0.918000    0.1040   0.3970  169.980               3   \n",
      "4         0.989          0.130000    0.3110   0.1960  103.220               4   \n",
      "5         0.994          0.247000    0.0977   0.0539  118.891               4   \n",
      "7         0.993          0.000016    0.3250   0.6540  133.088               3   \n",
      "\n",
      "                                              genres  \n",
      "2                             [tango, vintage tango]  \n",
      "3                             [tango, vintage tango]  \n",
      "4  [adult standards, easy listening, big band, lo...  \n",
      "5  [adult standards, easy listening, big band, lo...  \n",
      "7                                  [vintage chanson]  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "print(tracks.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9364f6fa-96a5-40dd-8d47-2a78482f8567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4706"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "all_genres = set(chain.from_iterable(tracks[\"genres\"]))\n",
    "len(all_genres)\n",
    "# all_genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af999b92-4010-4fb9-b5da-6f6cc67380a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_to_index = {genre: idx for idx, genre in enumerate(sorted(all_genres))}\n",
    "# genre_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f8f25fb-1407-42a6-9989-c04e25dd56c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multihot_vector(genres, index_dict):\n",
    "    multihot = [0] * len(index_dict)\n",
    "    for genre in genres:\n",
    "        multihot[index_dict[genre]] = 1\n",
    "    return multihot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "428fca29-7f55-4b3e-9537-e2ef8afb9573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "3         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "4         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "5         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "7         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "                                ...                        \n",
       "586667    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "586668    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "586669    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "586670    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "586671    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "Name: multi_hot_genres, Length: 499064, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracks[\"multi_hot_genres\"] = tracks[\"genres\"].apply(\n",
    "    lambda genres: multihot_vector(genres, genre_to_index)\n",
    ")\n",
    "tracks[\"multi_hot_genres\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d5aedaf-66d2-4faf-aa2e-bbe8c2504aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = tracks[[\"name\", \"popularity\", \"duration_ms\", \"explicit\", \"release_date\", \"danceability\", \"energy\", \"key\", \"loudness\", \"mode\", \"speechiness\", \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\", \"tempo\", \"time_signature\"]]\n",
    "y = tracks[\"multi_hot_genres\"].tolist()\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=478)\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=478)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34f732a6-c0f2-4d27-b749-2fa9ebef55b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 399251\n",
      "Dev set size: 49906\n",
      "Test set size: 49907\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train set size: {len(X_train)}\")\n",
    "print(f\"Dev set size: {len(X_dev)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc59b6ea-b98e-42d5-8a6c-91991e03982c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of labels: 4706\n",
      "Filtered number of labels: 1529\n"
     ]
    }
   ],
   "source": [
    "# Count label frequencies in the training set\n",
    "import numpy as np\n",
    "\n",
    "label_counts = np.sum(y_train, axis=0)\n",
    "\n",
    "# Set a threshold for filtering labels\n",
    "threshold =100\n",
    "selected_labels = np.where(label_counts >= threshold)[0]\n",
    "\n",
    "# Filter labels in train, dev, and test sets\n",
    "def filter_labels(y, selected_labels):\n",
    "    return np.array([[y_sample[i] for i in selected_labels] for y_sample in y])\n",
    "\n",
    "y_train = filter_labels(y_train, selected_labels)\n",
    "y_dev = filter_labels(y_dev, selected_labels)\n",
    "y_test = filter_labels(y_test, selected_labels)\n",
    "\n",
    "# Update genre_to_index mapping\n",
    "filtered_genre_to_index = {genre: idx for idx, genre in enumerate(selected_labels)}\n",
    "index_to_genre = {v: k for k, v in filtered_genre_to_index.items()}\n",
    "\n",
    "# Step 5: Print results for verification\n",
    "print(f\"Original number of labels: {len(label_counts)}\")\n",
    "print(f\"Filtered number of labels: {len(selected_labels)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9aa7530-f23d-4471-ac80-0e9c56d18d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size after filtering: 386974\n",
      "Dev set size after filtering: 48297\n",
      "Test set size after filtering: 48376\n"
     ]
    }
   ],
   "source": [
    "# Remove rows with no positive labels\n",
    "def remove_empty_labels(X, y):\n",
    "    non_empty_indices = [i for i, labels in enumerate(y) if np.sum(labels) > 0]\n",
    "    X_filtered = X.iloc[non_empty_indices].reset_index(drop=True)\n",
    "    y_filtered = np.array([y[i] for i in non_empty_indices])\n",
    "    return X_filtered, y_filtered\n",
    "\n",
    "# Apply to train, dev, and test sets\n",
    "X_train, y_train = remove_empty_labels(X_train, y_train)\n",
    "X_dev, y_dev = remove_empty_labels(X_dev, y_dev)\n",
    "X_test, y_test = remove_empty_labels(X_test, y_test)\n",
    "\n",
    "# Verify\n",
    "print(f\"Train set size after filtering: {len(X_train)}\")\n",
    "print(f\"Dev set size after filtering: {len(X_dev)}\")\n",
    "print(f\"Test set size after filtering: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f8a9644-ecff-49a1-a508-e5519cc64df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, jaccard_score, hamming_loss\n",
    "import numpy as np\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c31961ab-7c5e-4ca1-8ffc-434ae2406c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization function\n",
    "def tokenize_data(X):\n",
    "    combined_text = X.apply(lambda row: \" \".join(row.astype(str)), axis=1)\n",
    "    \n",
    "    # Tokenize the combined text\n",
    "    return tokenizer(\n",
    "        combined_text.tolist(),\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=16,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "# Dataset preparation\n",
    "class MultiLabelDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        tokenized = tokenize_data(X)\n",
    "\n",
    "        self.input_ids = tokenized[\"input_ids\"]\n",
    "        self.attention_mask = tokenized[\"attention_mask\"]\n",
    "        self.labels = torch.tensor(y)\n",
    "\n",
    "        token_lengths = [len(ids) for ids in self.input_ids]\n",
    "        print(f\"Token length range: {min(token_lengths)} to {max(token_lengths)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx],\n",
    "            \"labels\": self.labels[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "29b2fa67-2d35-48f0-bfcb-d1178c908f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token length range: 16 to 16\n",
      "Token length range: 16 to 16\n",
      "Token length range: 16 to 16\n"
     ]
    }
   ],
   "source": [
    "# Convert splits into datasets\n",
    "train_dataset = MultiLabelDataset(X_train, y_train)\n",
    "dev_dataset = MultiLabelDataset(X_dev, y_dev)\n",
    "test_dataset = MultiLabelDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b6f708e1-5d64-4f86-beca-030d73f16309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "class MultiLabelRoBERTa(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(MultiLabelRoBERTa, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.roberta.config.hidden_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        logits = self.classifier(outputs.pooler_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            criterion = nn.BCEWithLogitsLoss()\n",
    "            loss = criterion(logits, labels.float())\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "# Metrics computation\n",
    "def compute_metrics(pred, threshold_range=(0.05, 0.55, 0.05), prob=True):\n",
    "    logits, labels = pred\n",
    "    output = logits\n",
    "    if prob:\n",
    "        output = torch.sigmoid(torch.tensor(logits))\n",
    "\n",
    "    # Calculate logit statistics\n",
    "    prob_min = output.min().item()\n",
    "    prob_max = output.max().item()\n",
    "    prob_mean = output.mean().item()\n",
    "\n",
    "    def prob_threshold_pred(output, threshold):\n",
    "        if isinstance(output, np.ndarray):\n",
    "            return (output > threshold).astype(int)\n",
    "        return (output > threshold).int()\n",
    "\n",
    "    def calc_metrics(labels, predictions):\n",
    "        subset_accuracy = accuracy_score(labels, predictions)\n",
    "        precision = precision_score(labels, predictions, average=\"micro\")\n",
    "        recall = recall_score(labels, predictions, average=\"micro\")\n",
    "        f1 = f1_score(labels, predictions, average=\"micro\")\n",
    "        jaccard = jaccard_score(labels, predictions, average='samples')\n",
    "        hamming = hamming_loss(labels, predictions)\n",
    "\n",
    "        return subset_accuracy, precision, recall, f1, jaccard, hamming\n",
    "        \n",
    "    # Store results for thresholds\n",
    "    threshold_results = {}\n",
    "    for threshold in np.arange(*threshold_range):\n",
    "        predictions = prob_threshold_pred(output, threshold)\n",
    "        subset_accuracy, precision, recall, f1, jaccard, hamming = calc_metrics(labels, predictions)\n",
    "        threshold_results[threshold] = {\n",
    "            \"accuracy\": subset_accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "            \"hamming\": hamming,\n",
    "            \"jaccard\": jaccard\n",
    "        }\n",
    "\n",
    "    # Find optimal threshold\n",
    "    optimal_threshold = max(threshold_results, key=lambda t: threshold_results[t][\"f1\"])\n",
    "    optimal_threshold_metrics = threshold_results[optimal_threshold]\n",
    "\n",
    "    # Return detailed metrics\n",
    "    return {\n",
    "        \"prob_min\": prob_min,\n",
    "        \"prob_max\": prob_max,\n",
    "        \"prob_mean\": prob_mean,\n",
    "        \"optimal_threshold\": optimal_threshold,\n",
    "        \"optimal_threshold_accuracy\": optimal_threshold_metrics[\"accuracy\"],\n",
    "        \"optimal_threshold_precision\": optimal_threshold_metrics[\"precision\"],\n",
    "        \"optimal_threshold_recall\": optimal_threshold_metrics[\"recall\"],\n",
    "        \"optimal_threshold_f1\": optimal_threshold_metrics[\"f1\"],\n",
    "        \"optimal_threshold_hamming\": optimal_threshold_metrics[\"hamming\"],\n",
    "        \"optimal_threshold_jaccard\": optimal_threshold_metrics[\"jaccard\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c667123f-dd36-4247-a228-36607069bf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "def initialize_model():\n",
    "    num_labels = len(y_train[0])\n",
    "    model = MultiLabelRoBERTa(num_labels=num_labels)\n",
    "    model.roberta.gradient_checkpointing_enable()\n",
    "    return model\n",
    "model = initialize_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "075be418-828e-4c62-bb1a-f5bf0c8c7d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",\n",
    "#     eval_strategy=\"steps\",\n",
    "#     eval_steps=2000,\n",
    "#     save_strategy=\"steps\",\n",
    "#     save_steps=2000,\n",
    "#     learning_rate=1e-5,\n",
    "#     per_device_train_batch_size=16,\n",
    "#     # gradient_accumulation_steps=2,\n",
    "#     per_device_eval_batch_size=16,\n",
    "#     num_train_epochs=10,\n",
    "#     weight_decay=0.01,\n",
    "#     load_best_model_at_end=True,\n",
    "#     fp16=True\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "53111b9e-98ac-42ef-875b-187ddcf9e8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=dev_dataset,\n",
    "#     processing_class=tokenizer,\n",
    "#     compute_metrics=compute_metrics\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c5a70068-67c3-4ef7-a078-4f4977fb1bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the model\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f0a4f66a-b07c-46c7-bba8-f26d88542aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test set\n",
    "# trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2b1f6868-6ca4-4387-a882-ff56b5172897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_metrics(pred, top_k_range=(1, 10), threshold_range=(0.05, 0.5, 0.05)):\n",
    "#     logits, labels = pred\n",
    "\n",
    "#     # Calculate logit statistics\n",
    "#     logit_min = logits.min()\n",
    "#     logit_max = logits.max()\n",
    "#     logit_mean = logits.mean()\n",
    "\n",
    "#     def top_k_pred(logits, k):\n",
    "#         top_k_indices = np.argsort(-logits, axis=1)[:, :k]\n",
    "#         predictions = np.zeros_like(logits, dtype=int)\n",
    "#         for i, indices in enumerate(top_k_indices):\n",
    "#             predictions[i, indices] = 1\n",
    "#         return predictions\n",
    "\n",
    "#     def logit_threshold_pred(logits, threshold):\n",
    "#         return (logits > threshold).astype(int)\n",
    "\n",
    "#     def calc_metrics(labels, predictions):\n",
    "#         subset_accuracy = accuracy_score(labels, predictions)\n",
    "#         precision = precision_score(labels, predictions, average=\"micro\")\n",
    "#         recall = recall_score(labels, predictions, average=\"micro\")\n",
    "#         f1 = f1_score(labels, predictions, average=\"micro\")\n",
    "#         return subset_accuracy, precision, recall, f1\n",
    "\n",
    "#     # Store results for top-k\n",
    "#     top_k_results = {}\n",
    "#     for k in range(*top_k_range):\n",
    "#         predictions = top_k_pred(logits, k)\n",
    "#         subset_accuracy, precision, recall, f1 = calc_metrics(labels, predictions)\n",
    "#         top_k_results[k] = {\n",
    "#             \"accuracy\": subset_accuracy,\n",
    "#             \"precision\": precision,\n",
    "#             \"recall\": recall,\n",
    "#             \"f1\": f1\n",
    "#         }\n",
    "\n",
    "#     # Find optimal k\n",
    "#     optimal_k = max(top_k_results, key=lambda k: top_k_results[k][\"f1\"])\n",
    "#     optimal_k_metrics = top_k_results[optimal_k]\n",
    "\n",
    "#     # Store results for thresholds\n",
    "#     threshold_results = {}\n",
    "#     for threshold in np.arange(*threshold_range):\n",
    "#         predictions = logit_threshold_pred(logits, threshold)\n",
    "#         subset_accuracy, precision, recall, f1 = calc_metrics(labels, predictions)\n",
    "#         threshold_results[threshold] = {\n",
    "#             \"accuracy\": subset_accuracy,\n",
    "#             \"precision\": precision,\n",
    "#             \"recall\": recall,\n",
    "#             \"f1\": f1\n",
    "#         }\n",
    "\n",
    "#     # Find optimal threshold\n",
    "#     optimal_threshold = max(threshold_results, key=lambda t: threshold_results[t][\"f1\"])\n",
    "#     optimal_threshold_metrics = threshold_results[optimal_threshold]\n",
    "\n",
    "#     # Return detailed metrics\n",
    "#     return {\n",
    "#         \"logit_min\": logit_min,\n",
    "#         \"logit_max\": logit_max,\n",
    "#         \"logit_mean\": logit_mean,\n",
    "#         \"optimal_k\": optimal_k,\n",
    "#         \"optimal_k_accuracy\": optimal_k_metrics[\"accuracy\"],\n",
    "#         \"optimal_k_precision\": optimal_k_metrics[\"precision\"],\n",
    "#         \"optimal_k_recall\": optimal_k_metrics[\"recall\"],\n",
    "#         \"optimal_k_f1\": optimal_k_metrics[\"f1\"],\n",
    "#         \"optimal_threshold\": optimal_threshold,\n",
    "#         \"optimal_threshold_accuracy\": optimal_threshold_metrics[\"accuracy\"],\n",
    "#         \"optimal_threshold_precision\": optimal_threshold_metrics[\"precision\"],\n",
    "#         \"optimal_threshold_recall\": optimal_threshold_metrics[\"recall\"],\n",
    "#         \"optimal_threshold_f1\": optimal_threshold_metrics[\"f1\"]\n",
    "#     }\n",
    "\n",
    "\n",
    "\n",
    "# # Initialize the model\n",
    "# num_labels = len(y_train[0])\n",
    "# model = MultiLabelRoBERTa(num_labels=num_labels)\n",
    "# model.roberta.gradient_checkpointing_enable()\n",
    "# # Training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",\n",
    "#     eval_strategy=\"steps\",\n",
    "#     eval_steps=4000,\n",
    "#     save_strategy=\"steps\",\n",
    "#     save_steps=4000,\n",
    "#     learning_rate=1e-5,\n",
    "#     per_device_train_batch_size=32,\n",
    "#     # gradient_accumulation_steps=2,\n",
    "#     per_device_eval_batch_size=16,\n",
    "#     num_train_epochs=20,\n",
    "#     weight_decay=0.01,\n",
    "#     load_best_model_at_end=True,\n",
    "#     fp16=True\n",
    "# )\n",
    "# # Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=dev_dataset,\n",
    "#     processing_class=tokenizer,\n",
    "#     compute_metrics=compute_metrics\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5f187b57-4f24-4b56-857a-e6dcfed62751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c7962725-e355-49f4-a9f7-3335282eb031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test set\n",
    "# trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bd51730f-5e31-4b94-b189-55d84a9fc51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Random chance baseline\n",
    "# label_frequencies = y_train.mean(axis=0)\n",
    "# random_predictions = np.random.rand(*y_dev.shape) < label_frequencies\n",
    "# random_baseline_metrics = compute_metrics((random_predictions, y_dev), prob=False)\n",
    "# print(\"Random Baseline Metrics:\", random_baseline_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "867df1a6-7958-43a5-a0ad-3a632ad16745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# # Naive Bayes baseline\n",
    "# # Flatten multi-label data for Naive Bayes (treat each label as independent)\n",
    "# X_train_NB = X_train\n",
    "# X_dev_NB = X_dev\n",
    "\n",
    "# vectorizer = CountVectorizer()\n",
    "# X_train_vectorized = vectorizer.fit_transform(X_train_NB)\n",
    "# X_dev_vectorized = vectorizer.transform(X_dev_NB)\n",
    "\n",
    "# # Train separate Naive Bayes for each label\n",
    "# naive_bayes_predictions = []\n",
    "# for i in range(y_train.shape[1]):\n",
    "#     model = MultinomialNB()\n",
    "#     model.fit(X_dev_vectorized, y_train[:, i])\n",
    "#     predictions = model.predict(X_dev_vectorized)\n",
    "#     naive_bayes_predictions.append(predictions)\n",
    "\n",
    "# naive_bayes_predictions = np.array(naive_bayes_predictions).T  # Convert to (samples x labels)\n",
    "# naive_bayes_baseline_metrics = compute_metrics((naive_bayes_predictions, y_test))\n",
    "# print(\"Naive Bayes Baseline Metrics:\", naive_bayes_baseline_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd4e3eb-e5c6-43a0-8103-b3d4c7d54ad2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1/5 with parameters: {'weight_decay': 0.01, 'per_device_train_batch_size': 64, 'num_train_epochs': 5, 'learning_rate': 1e-05}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30235' max='30235' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30235/30235 1:00:56, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Prob Min</th>\n",
       "      <th>Prob Max</th>\n",
       "      <th>Prob Mean</th>\n",
       "      <th>Optimal Threshold</th>\n",
       "      <th>Optimal Threshold Accuracy</th>\n",
       "      <th>Optimal Threshold Precision</th>\n",
       "      <th>Optimal Threshold Recall</th>\n",
       "      <th>Optimal Threshold F1</th>\n",
       "      <th>Optimal Threshold Hamming</th>\n",
       "      <th>Optimal Threshold Jaccard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.018500</td>\n",
       "      <td>0.014940</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.209955</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000973</td>\n",
       "      <td>0.247967</td>\n",
       "      <td>0.076125</td>\n",
       "      <td>0.116488</td>\n",
       "      <td>0.002863</td>\n",
       "      <td>0.038178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.014200</td>\n",
       "      <td>0.012480</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.816113</td>\n",
       "      <td>0.001485</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.023811</td>\n",
       "      <td>0.234469</td>\n",
       "      <td>0.265232</td>\n",
       "      <td>0.248904</td>\n",
       "      <td>0.003969</td>\n",
       "      <td>0.131342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.012800</td>\n",
       "      <td>0.011364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.927757</td>\n",
       "      <td>0.001695</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.027600</td>\n",
       "      <td>0.312335</td>\n",
       "      <td>0.260852</td>\n",
       "      <td>0.284281</td>\n",
       "      <td>0.003256</td>\n",
       "      <td>0.151378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.012100</td>\n",
       "      <td>0.010884</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.955817</td>\n",
       "      <td>0.001748</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.034992</td>\n",
       "      <td>0.320682</td>\n",
       "      <td>0.297114</td>\n",
       "      <td>0.308449</td>\n",
       "      <td>0.003303</td>\n",
       "      <td>0.176465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.011900</td>\n",
       "      <td>0.010721</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.960510</td>\n",
       "      <td>0.001789</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.036089</td>\n",
       "      <td>0.315552</td>\n",
       "      <td>0.310824</td>\n",
       "      <td>0.313170</td>\n",
       "      <td>0.003380</td>\n",
       "      <td>0.182378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3019' max='3019' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3019/3019 00:41]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 F1 Score: 0.3131558776007632\n",
      "Trial 2/5 with parameters: {'weight_decay': 0.01, 'per_device_train_batch_size': 64, 'num_train_epochs': 10, 'learning_rate': 1e-05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60470' max='60470' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60470/60470 1:59:48, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Prob Min</th>\n",
       "      <th>Prob Max</th>\n",
       "      <th>Prob Mean</th>\n",
       "      <th>Optimal Threshold</th>\n",
       "      <th>Optimal Threshold Accuracy</th>\n",
       "      <th>Optimal Threshold Precision</th>\n",
       "      <th>Optimal Threshold Recall</th>\n",
       "      <th>Optimal Threshold F1</th>\n",
       "      <th>Optimal Threshold Hamming</th>\n",
       "      <th>Optimal Threshold Jaccard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.018300</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.241311</td>\n",
       "      <td>0.001188</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000828</td>\n",
       "      <td>0.272867</td>\n",
       "      <td>0.088950</td>\n",
       "      <td>0.134164</td>\n",
       "      <td>0.002846</td>\n",
       "      <td>0.044313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.013600</td>\n",
       "      <td>0.011960</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.876529</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.013210</td>\n",
       "      <td>0.235754</td>\n",
       "      <td>0.316341</td>\n",
       "      <td>0.270166</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.150695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>0.010708</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.948537</td>\n",
       "      <td>0.001707</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.028946</td>\n",
       "      <td>0.318001</td>\n",
       "      <td>0.309579</td>\n",
       "      <td>0.313734</td>\n",
       "      <td>0.003358</td>\n",
       "      <td>0.180470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.011100</td>\n",
       "      <td>0.010059</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.971019</td>\n",
       "      <td>0.001817</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.038843</td>\n",
       "      <td>0.315822</td>\n",
       "      <td>0.359300</td>\n",
       "      <td>0.336161</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.206108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.010600</td>\n",
       "      <td>0.009663</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.980655</td>\n",
       "      <td>0.001863</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.049092</td>\n",
       "      <td>0.392637</td>\n",
       "      <td>0.320104</td>\n",
       "      <td>0.352680</td>\n",
       "      <td>0.002913</td>\n",
       "      <td>0.213795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>0.009389</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.987277</td>\n",
       "      <td>0.001943</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.052198</td>\n",
       "      <td>0.390196</td>\n",
       "      <td>0.345175</td>\n",
       "      <td>0.366307</td>\n",
       "      <td>0.002961</td>\n",
       "      <td>0.227992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.009219</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.989511</td>\n",
       "      <td>0.001973</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.052778</td>\n",
       "      <td>0.385091</td>\n",
       "      <td>0.364156</td>\n",
       "      <td>0.374331</td>\n",
       "      <td>0.003018</td>\n",
       "      <td>0.236703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.009800</td>\n",
       "      <td>0.009102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.991748</td>\n",
       "      <td>0.001966</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.052860</td>\n",
       "      <td>0.389809</td>\n",
       "      <td>0.372229</td>\n",
       "      <td>0.380816</td>\n",
       "      <td>0.003001</td>\n",
       "      <td>0.240585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.009700</td>\n",
       "      <td>0.009027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.992823</td>\n",
       "      <td>0.002007</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.052985</td>\n",
       "      <td>0.386704</td>\n",
       "      <td>0.381831</td>\n",
       "      <td>0.384252</td>\n",
       "      <td>0.003034</td>\n",
       "      <td>0.245380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.009600</td>\n",
       "      <td>0.009003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.993307</td>\n",
       "      <td>0.001985</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.053502</td>\n",
       "      <td>0.390462</td>\n",
       "      <td>0.382443</td>\n",
       "      <td>0.386411</td>\n",
       "      <td>0.003011</td>\n",
       "      <td>0.246404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3019' max='3019' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3019/3019 00:43]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 F1 Score: 0.3865365111561866\n",
      "Trial 3/5 with parameters: {'weight_decay': 0.01, 'per_device_train_batch_size': 128, 'num_train_epochs': 10, 'learning_rate': 1e-05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14286' max='30240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14286/30240 2:33:59 < 2:51:59, 1.55 it/s, Epoch 4.72/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Prob Min</th>\n",
       "      <th>Prob Max</th>\n",
       "      <th>Prob Mean</th>\n",
       "      <th>Optimal Threshold</th>\n",
       "      <th>Optimal Threshold Accuracy</th>\n",
       "      <th>Optimal Threshold Precision</th>\n",
       "      <th>Optimal Threshold Recall</th>\n",
       "      <th>Optimal Threshold F1</th>\n",
       "      <th>Optimal Threshold Hamming</th>\n",
       "      <th>Optimal Threshold Jaccard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.017800</td>\n",
       "      <td>0.014531</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.272315</td>\n",
       "      <td>0.001176</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000766</td>\n",
       "      <td>0.275984</td>\n",
       "      <td>0.109137</td>\n",
       "      <td>0.156419</td>\n",
       "      <td>0.002918</td>\n",
       "      <td>0.055438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.013300</td>\n",
       "      <td>0.011669</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.877161</td>\n",
       "      <td>0.001597</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.021740</td>\n",
       "      <td>0.234866</td>\n",
       "      <td>0.349485</td>\n",
       "      <td>0.280934</td>\n",
       "      <td>0.004435</td>\n",
       "      <td>0.165884</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterSampler\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Define hyperparameter search space\n",
    "param_grid = {\n",
    "    \"learning_rate\": [1e-5],\n",
    "    \"num_train_epochs\": [5, 10],\n",
    "    \"per_device_train_batch_size\": [32, 64, 128],\n",
    "    \"weight_decay\": [0.01]\n",
    "}\n",
    "\n",
    "# Generate random samples of hyperparameters\n",
    "n_trials = 5\n",
    "param_samples = list(ParameterSampler(param_grid, n_iter=n_trials, random_state=478))\n",
    "\n",
    "best_score = -np.inf\n",
    "best_params = None\n",
    "best_model = None\n",
    "\n",
    "# Iterate over sampled hyperparameters\n",
    "for i, params in enumerate(param_samples):\n",
    "    print(f\"Trial {i+1}/{n_trials} with parameters: {params}\")\n",
    "\n",
    "    model = initialize_model()\n",
    "\n",
    "    # Update TrainingArguments dynamically\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results/trial_{i}\",\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=6000,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=6000,\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        per_device_train_batch_size=params[\"per_device_train_batch_size\"],\n",
    "        per_device_eval_batch_size=16, \n",
    "        num_train_epochs=params[\"num_train_epochs\"],\n",
    "        weight_decay=params[\"weight_decay\"],\n",
    "        fp16=True  # Mixed precision for speed\n",
    "    )\n",
    "\n",
    "    # Define the Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=dev_dataset,\n",
    "        processing_class=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate on the validation set\n",
    "    metrics = trainer.evaluate(dev_dataset)\n",
    "    f1 = metrics[\"eval_optimal_threshold_f1\"]\n",
    "\n",
    "    print(f\"Trial {i+1} F1 Score: {f1}\")\n",
    "\n",
    "    # Keep track of the best model\n",
    "    if f1 > best_score:\n",
    "        best_score = f1\n",
    "        best_params = params\n",
    "        best_model = trainer\n",
    "        trainer.save_model(\"./models\")\n",
    "\n",
    "# Output the best results\n",
    "print(f\"Best F1 Score: {best_score}\")\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "# Save the best model\n",
    "best_model.save_model(\"./best-model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77f6a58-ca72-44c8-a214-1e2d991b52ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterSampler\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Define hyperparameter search space\n",
    "param_grid = {\n",
    "    \"learning_rate\": [1e-5],\n",
    "    \"num_train_epochs\": [5, 10],\n",
    "    \"per_device_train_batch_size\": [32, 128],\n",
    "    \"weight_decay\": [0.01]\n",
    "}\n",
    "\n",
    "# Generate random samples of hyperparameters\n",
    "n_trials = 3\n",
    "param_samples = list(ParameterSampler(param_grid, n_iter=n_trials, random_state=478))\n",
    "\n",
    "best_score = -np.inf\n",
    "best_params = None\n",
    "best_model = None\n",
    "\n",
    "# Iterate over sampled hyperparameters\n",
    "for i, params in enumerate(param_samples):\n",
    "    print(f\"Trial {i+1}/{n_trials} with parameters: {params}\")\n",
    "\n",
    "    model = initialize_model()\n",
    "\n",
    "    # Update TrainingArguments dynamically\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results/trial_{i}\",\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=6000,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=6000,\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        per_device_train_batch_size=params[\"per_device_train_batch_size\"],\n",
    "        per_device_eval_batch_size=16, \n",
    "        num_train_epochs=params[\"num_train_epochs\"],\n",
    "        weight_decay=params[\"weight_decay\"],\n",
    "        fp16=True  # Mixed precision for speed\n",
    "    )\n",
    "\n",
    "    # Define the Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=dev_dataset,\n",
    "        processing_class=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate on the validation set\n",
    "    metrics = trainer.evaluate(dev_dataset)\n",
    "    f1 = metrics[\"eval_optimal_threshold_f1\"]\n",
    "\n",
    "    print(f\"Trial {i+1} F1 Score: {f1}\")\n",
    "    print(metrics)\n",
    "\n",
    "    # Keep track of the best model\n",
    "    if f1 > best_score:\n",
    "        best_score = f1\n",
    "        best_params = params\n",
    "        best_model = trainer\n",
    "        trainer.save_model(\"./models\")\n",
    "\n",
    "# Output the best results\n",
    "print(f\"Best F1 Score: {best_score}\")\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "# Save the best model\n",
    "best_model.save_model(\"./best-model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2a9f46-3eaa-48bc-91d1-248ee8811218",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e69a03d-58b2-44f7-8688-843b5b4e6913",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
