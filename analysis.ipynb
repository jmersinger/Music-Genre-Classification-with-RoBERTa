{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a72bd69-a821-4a47-af97-b1ad2170fc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Allocated GPU memory: 0.00 GB\n",
      "Reserved GPU memory: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Optionally allocate a fraction of GPU memory\n",
    "torch.cuda.set_per_process_memory_fraction(0.9, 0)\n",
    "\n",
    "# Check memory stats\n",
    "print(f'Allocated GPU memory: {torch.cuda.memory_allocated(device) / (1024 ** 3):.2f} GB')\n",
    "print(f'Reserved GPU memory: {torch.cuda.memory_reserved(device) / (1024 ** 3):.2f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "175073ea-4071-4aaf-a332-e354846b474c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.cleaning import download_and_clean\n",
    "\n",
    "artists, tracks = download_and_clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d54a8e77-2193-4fe4-8043-282cc51d056f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         id  followers                      genres  \\\n",
      "45   0VLMVnVbJyJ4oyZs2L3Yl2       71.0          ['carnaval cadiz']   \n",
      "46   0dt23bs4w8zx154C5xdVyl       63.0          ['carnaval cadiz']   \n",
      "47   0pGhoB99qpEJEsBQxgaskQ       64.0          ['carnaval cadiz']   \n",
      "48   3HDrX2OtSuXLW5dLR85uN3       53.0          ['carnaval cadiz']   \n",
      "136  22mLrN5fkppmuUPsHx6i2G       59.0  ['classical harp', 'harp']   \n",
      "\n",
      "                             name  popularity  \n",
      "45   Las Viudas De Los Bisabuelos           6  \n",
      "46              Los De Capuchinos           5  \n",
      "47             Los “Pofesionales”           7  \n",
      "48      Los Que No Paran De Rajar           6  \n",
      "136                   Vera Dulova           3  \n"
     ]
    }
   ],
   "source": [
    "print(artists.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07ae631a-3033-429d-8484-1f0d06c826a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       id                                name  popularity  \\\n",
      "2  07A5yehtSnoedViJAZkNnc  Vivo para Quererte - Remasterizado           0   \n",
      "3  08FmqUhxtyLTn6pAh6bk45       El Prisionero - Remasterizado           0   \n",
      "4  08y9GfoqCWfOGsKdwojr5e                 Lady of the Evening           0   \n",
      "5  0BRXJHRNGQ3W4v9frnSfhu                           Ave Maria           0   \n",
      "7  0IA0Hju8CAgYfV1hwhidBH                             La Java           0   \n",
      "\n",
      "   duration_ms  explicit              artists                  id_artists  \\\n",
      "2       181640         0  ['Ignacio Corsini']  ['5LiOoJbxVSAMkBS2fUm3X2']   \n",
      "3       176907         0  ['Ignacio Corsini']  ['5LiOoJbxVSAMkBS2fUm3X2']   \n",
      "4       163080         0      ['Dick Haymes']  ['3BiJGZsyX9sJchTqcSA7Su']   \n",
      "5       178933         0      ['Dick Haymes']  ['3BiJGZsyX9sJchTqcSA7Su']   \n",
      "7       161427         0      ['Mistinguett']  ['4AxgXfD7ISvJSTObqm4aIE']   \n",
      "\n",
      "  release_date  danceability  energy  ...  loudness  mode  speechiness  \\\n",
      "2   1922-03-21         0.434  0.1770  ...   -21.180     1       0.0512   \n",
      "3   1922-03-21         0.321  0.0946  ...   -27.961     1       0.0504   \n",
      "4         1922         0.402  0.1580  ...   -16.900     0       0.0390   \n",
      "5         1922         0.227  0.2610  ...   -12.343     1       0.0382   \n",
      "7         1922         0.563  0.1840  ...   -13.757     1       0.0512   \n",
      "\n",
      "   acousticness  instrumentalness  liveness  valence    tempo  time_signature  \\\n",
      "2         0.994          0.021800    0.2120   0.4570  130.418               5   \n",
      "3         0.995          0.918000    0.1040   0.3970  169.980               3   \n",
      "4         0.989          0.130000    0.3110   0.1960  103.220               4   \n",
      "5         0.994          0.247000    0.0977   0.0539  118.891               4   \n",
      "7         0.993          0.000016    0.3250   0.6540  133.088               3   \n",
      "\n",
      "                                              genres  \n",
      "2                             [vintage tango, tango]  \n",
      "3                             [vintage tango, tango]  \n",
      "4  [easy listening, swing, big band, adult standa...  \n",
      "5  [easy listening, swing, big band, adult standa...  \n",
      "7                                  [vintage chanson]  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "print(tracks.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9364f6fa-96a5-40dd-8d47-2a78482f8567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4706"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "all_genres = set(chain.from_iterable(tracks[\"genres\"]))\n",
    "len(all_genres)\n",
    "# all_genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af999b92-4010-4fb9-b5da-6f6cc67380a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_to_index = {genre: idx for idx, genre in enumerate(sorted(all_genres))}\n",
    "# genre_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f8f25fb-1407-42a6-9989-c04e25dd56c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multihot_vector(genres, index_dict):\n",
    "    multihot = [0] * len(index_dict)\n",
    "    for genre in genres:\n",
    "        multihot[index_dict[genre]] = 1\n",
    "    return multihot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "428fca29-7f55-4b3e-9537-e2ef8afb9573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "3         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "4         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "5         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "7         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "                                ...                        \n",
       "586667    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "586668    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "586669    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "586670    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "586671    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "Name: multi_hot_genres, Length: 499064, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracks[\"multi_hot_genres\"] = tracks[\"genres\"].apply(\n",
    "    lambda genres: multihot_vector(genres, genre_to_index)\n",
    ")\n",
    "tracks[\"multi_hot_genres\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d5aedaf-66d2-4faf-aa2e-bbe8c2504aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = tracks[[\"name\", \"popularity\", \"duration_ms\", \"explicit\", \"release_date\", \"danceability\", \"energy\", \"key\", \"loudness\", \"mode\", \"speechiness\", \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\", \"tempo\", \"time_signature\"]]\n",
    "y = tracks[\"multi_hot_genres\"].tolist()\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=478)\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=478)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34f732a6-c0f2-4d27-b749-2fa9ebef55b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 399251\n",
      "Dev set size: 49906\n",
      "Test set size: 49907\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train set size: {len(X_train)}\")\n",
    "print(f\"Dev set size: {len(X_dev)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc59b6ea-b98e-42d5-8a6c-91991e03982c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of labels: 4706\n",
      "Filtered number of labels: 1529\n"
     ]
    }
   ],
   "source": [
    "# Count label frequencies in the training set\n",
    "import numpy as np\n",
    "\n",
    "label_counts = np.sum(y_train, axis=0)\n",
    "\n",
    "# Set a threshold for filtering labels\n",
    "threshold =100\n",
    "selected_labels = np.where(label_counts >= threshold)[0]\n",
    "\n",
    "# Filter labels in train, dev, and test sets\n",
    "def filter_labels(y, selected_labels):\n",
    "    return np.array([[y_sample[i] for i in selected_labels] for y_sample in y])\n",
    "\n",
    "y_train = filter_labels(y_train, selected_labels)\n",
    "y_dev = filter_labels(y_dev, selected_labels)\n",
    "y_test = filter_labels(y_test, selected_labels)\n",
    "\n",
    "# Update genre_to_index mapping\n",
    "filtered_genre_to_index = {genre: idx for idx, genre in enumerate(selected_labels)}\n",
    "index_to_genre = {v: k for k, v in filtered_genre_to_index.items()}\n",
    "\n",
    "# Step 5: Print results for verification\n",
    "print(f\"Original number of labels: {len(label_counts)}\")\n",
    "print(f\"Filtered number of labels: {len(selected_labels)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9aa7530-f23d-4471-ac80-0e9c56d18d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size after filtering: 386974\n",
      "Dev set size after filtering: 48297\n",
      "Test set size after filtering: 48376\n"
     ]
    }
   ],
   "source": [
    "# Remove rows with no positive labels\n",
    "def remove_empty_labels(X, y):\n",
    "    non_empty_indices = [i for i, labels in enumerate(y) if np.sum(labels) > 0]\n",
    "    X_filtered = X.iloc[non_empty_indices].reset_index(drop=True)\n",
    "    y_filtered = np.array([y[i] for i in non_empty_indices])\n",
    "    return X_filtered, y_filtered\n",
    "\n",
    "# Apply to train, dev, and test sets\n",
    "X_train, y_train = remove_empty_labels(X_train, y_train)\n",
    "X_dev, y_dev = remove_empty_labels(X_dev, y_dev)\n",
    "X_test, y_test = remove_empty_labels(X_test, y_test)\n",
    "\n",
    "# Verify\n",
    "print(f\"Train set size after filtering: {len(X_train)}\")\n",
    "print(f\"Dev set size after filtering: {len(X_dev)}\")\n",
    "print(f\"Test set size after filtering: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f8a9644-ecff-49a1-a508-e5519cc64df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c31961ab-7c5e-4ca1-8ffc-434ae2406c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization function\n",
    "def tokenize_data(X):\n",
    "    return tokenizer(\n",
    "        list(X[\"name\"]),\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=64,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "# Dataset preparation\n",
    "class MultiLabelDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        tokenized = tokenize_data(X)\n",
    "        self.input_ids = tokenized[\"input_ids\"]\n",
    "        self.attention_mask = tokenized[\"attention_mask\"]\n",
    "        self.labels = torch.tensor(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx],\n",
    "            \"labels\": self.labels[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29b2fa67-2d35-48f0-bfcb-d1178c908f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert splits into datasets\n",
    "train_dataset = MultiLabelDataset(X_train, y_train)\n",
    "dev_dataset = MultiLabelDataset(X_dev, y_dev)\n",
    "test_dataset = MultiLabelDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6f708e1-5d64-4f86-beca-030d73f16309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "class MultiLabelRoBERTa(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(MultiLabelRoBERTa, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.roberta.config.hidden_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        logits = self.classifier(outputs.pooler_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            criterion = nn.BCEWithLogitsLoss()\n",
    "            loss = criterion(logits, labels.float())\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "# Metrics computation\n",
    "def compute_metrics(pred, top_k_range=(1, 5), threshold_range=(0.1, 0.5, 0.05)):\n",
    "    logits, labels = pred\n",
    "\n",
    "    # Calculate logit statistics\n",
    "    logit_min = logits.min()\n",
    "    logit_max = logits.max()\n",
    "    logit_mean = logits.mean()\n",
    "\n",
    "    def top_k_pred(logits, k):\n",
    "        top_k_indices = np.argsort(-logits, axis=1)[:, :k]\n",
    "        predictions = np.zeros_like(logits, dtype=int)\n",
    "        for i, indices in enumerate(top_k_indices):\n",
    "            predictions[i, indices] = 1\n",
    "        return predictions\n",
    "\n",
    "    def logit_threshold_pred(logits, threshold):\n",
    "        return (logits > threshold).astype(int)\n",
    "\n",
    "    def calc_metrics(labels, predictions):\n",
    "        subset_accuracy = accuracy_score(labels, predictions)\n",
    "        precision = precision_score(labels, predictions, average=\"micro\")\n",
    "        recall = recall_score(labels, predictions, average=\"micro\")\n",
    "        f1 = f1_score(labels, predictions, average=\"micro\")\n",
    "        return subset_accuracy, precision, recall, f1\n",
    "\n",
    "    # Store results for top-k\n",
    "    top_k_results = {}\n",
    "    for k in range(*top_k_range):\n",
    "        predictions = top_k_pred(logits, k)\n",
    "        subset_accuracy, precision, recall, f1 = calc_metrics(labels, predictions)\n",
    "        top_k_results[k] = {\n",
    "            \"accuracy\": subset_accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1\n",
    "        }\n",
    "\n",
    "    # Find optimal k\n",
    "    optimal_k = max(top_k_results, key=lambda k: top_k_results[k][\"f1\"])\n",
    "    optimal_k_metrics = top_k_results[optimal_k]\n",
    "\n",
    "    # Store results for thresholds\n",
    "    threshold_results = {}\n",
    "    for threshold in np.arange(*threshold_range):\n",
    "        predictions = logit_threshold_pred(logits, threshold)\n",
    "        subset_accuracy, precision, recall, f1 = calc_metrics(labels, predictions)\n",
    "        threshold_results[threshold] = {\n",
    "            \"accuracy\": subset_accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1\n",
    "        }\n",
    "\n",
    "    # Find optimal threshold\n",
    "    optimal_threshold = max(threshold_results, key=lambda t: threshold_results[t][\"f1\"])\n",
    "    optimal_threshold_metrics = threshold_results[optimal_threshold]\n",
    "\n",
    "    # Return detailed metrics\n",
    "    return {\n",
    "        \"logit_min\": logit_min,\n",
    "        \"logit_max\": logit_max,\n",
    "        \"logit_mean\": logit_mean,\n",
    "        \"optimal_k\": optimal_k,\n",
    "        \"optimal_k_accuracy\": optimal_k_metrics[\"accuracy\"],\n",
    "        \"optimal_k_precision\": optimal_k_metrics[\"precision\"],\n",
    "        \"optimal_k_recall\": optimal_k_metrics[\"recall\"],\n",
    "        \"optimal_k_f1\": optimal_k_metrics[\"f1\"],\n",
    "        \"optimal_threshold\": optimal_threshold,\n",
    "        \"optimal_threshold_accuracy\": optimal_threshold_metrics[\"accuracy\"],\n",
    "        \"optimal_threshold_precision\": optimal_threshold_metrics[\"precision\"],\n",
    "        \"optimal_threshold_recall\": optimal_threshold_metrics[\"recall\"],\n",
    "        \"optimal_threshold_f1\": optimal_threshold_metrics[\"f1\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c667123f-dd36-4247-a228-36607069bf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "num_labels = len(y_train[0])\n",
    "model = MultiLabelRoBERTa(num_labels=num_labels)\n",
    "model.roberta.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "075be418-828e-4c62-bb1a-f5bf0c8c7d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=2000,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=2000,\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    # gradient_accumulation_steps=2,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53111b9e-98ac-42ef-875b-187ddcf9e8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a70068-67c3-4ef7-a078-4f4977fb1bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='140098' max='241860' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [140098/241860 6:12:46 < 4:30:46, 6.26 it/s, Epoch 5.79/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Logit Min</th>\n",
       "      <th>Logit Max</th>\n",
       "      <th>Logit Mean</th>\n",
       "      <th>Optimal K</th>\n",
       "      <th>Optimal K Accuracy</th>\n",
       "      <th>Optimal K Precision</th>\n",
       "      <th>Optimal K Recall</th>\n",
       "      <th>Optimal K F1</th>\n",
       "      <th>Optimal Threshold</th>\n",
       "      <th>Optimal Threshold Accuracy</th>\n",
       "      <th>Optimal Threshold Precision</th>\n",
       "      <th>Optimal Threshold Recall</th>\n",
       "      <th>Optimal Threshold F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.040300</td>\n",
       "      <td>0.018582</td>\n",
       "      <td>-15.164062</td>\n",
       "      <td>-2.964844</td>\n",
       "      <td>-7.870438</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003613</td>\n",
       "      <td>0.001906</td>\n",
       "      <td>0.002496</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.023100</td>\n",
       "      <td>0.016554</td>\n",
       "      <td>-14.773438</td>\n",
       "      <td>-2.775391</td>\n",
       "      <td>-7.605056</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056494</td>\n",
       "      <td>0.059613</td>\n",
       "      <td>0.058012</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.019600</td>\n",
       "      <td>0.016012</td>\n",
       "      <td>-14.039062</td>\n",
       "      <td>-2.554688</td>\n",
       "      <td>-7.637031</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.072717</td>\n",
       "      <td>0.057548</td>\n",
       "      <td>0.064249</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>0.015303</td>\n",
       "      <td>-12.546875</td>\n",
       "      <td>-1.671875</td>\n",
       "      <td>-7.577580</td>\n",
       "      <td>4</td>\n",
       "      <td>0.002547</td>\n",
       "      <td>0.106073</td>\n",
       "      <td>0.111929</td>\n",
       "      <td>0.108922</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.016900</td>\n",
       "      <td>0.014576</td>\n",
       "      <td>-13.148438</td>\n",
       "      <td>-0.241089</td>\n",
       "      <td>-7.548992</td>\n",
       "      <td>4</td>\n",
       "      <td>0.002733</td>\n",
       "      <td>0.120836</td>\n",
       "      <td>0.127506</td>\n",
       "      <td>0.124081</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>0.013851</td>\n",
       "      <td>-13.234375</td>\n",
       "      <td>1.361328</td>\n",
       "      <td>-7.413457</td>\n",
       "      <td>4</td>\n",
       "      <td>0.002837</td>\n",
       "      <td>0.136266</td>\n",
       "      <td>0.143789</td>\n",
       "      <td>0.139926</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.903295</td>\n",
       "      <td>0.006888</td>\n",
       "      <td>0.013671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.015500</td>\n",
       "      <td>0.013434</td>\n",
       "      <td>-14.781250</td>\n",
       "      <td>1.993164</td>\n",
       "      <td>-7.382252</td>\n",
       "      <td>4</td>\n",
       "      <td>0.002754</td>\n",
       "      <td>0.145377</td>\n",
       "      <td>0.153402</td>\n",
       "      <td>0.149281</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>0.880412</td>\n",
       "      <td>0.013994</td>\n",
       "      <td>0.027550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.014600</td>\n",
       "      <td>0.013172</td>\n",
       "      <td>-15.617188</td>\n",
       "      <td>2.078125</td>\n",
       "      <td>-7.570065</td>\n",
       "      <td>4</td>\n",
       "      <td>0.002878</td>\n",
       "      <td>0.150392</td>\n",
       "      <td>0.158695</td>\n",
       "      <td>0.154432</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.004493</td>\n",
       "      <td>0.803779</td>\n",
       "      <td>0.026491</td>\n",
       "      <td>0.051292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>0.012815</td>\n",
       "      <td>-15.851562</td>\n",
       "      <td>2.355469</td>\n",
       "      <td>-7.632562</td>\n",
       "      <td>4</td>\n",
       "      <td>0.003064</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.172421</td>\n",
       "      <td>0.167790</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.017268</td>\n",
       "      <td>0.791369</td>\n",
       "      <td>0.035056</td>\n",
       "      <td>0.067137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.014000</td>\n",
       "      <td>0.012648</td>\n",
       "      <td>-16.531250</td>\n",
       "      <td>2.673828</td>\n",
       "      <td>-7.805795</td>\n",
       "      <td>4</td>\n",
       "      <td>0.002982</td>\n",
       "      <td>0.174436</td>\n",
       "      <td>0.184066</td>\n",
       "      <td>0.179122</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.017289</td>\n",
       "      <td>0.809737</td>\n",
       "      <td>0.034613</td>\n",
       "      <td>0.066388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.013800</td>\n",
       "      <td>0.012478</td>\n",
       "      <td>-17.453125</td>\n",
       "      <td>2.513672</td>\n",
       "      <td>-7.877540</td>\n",
       "      <td>4</td>\n",
       "      <td>0.003064</td>\n",
       "      <td>0.184240</td>\n",
       "      <td>0.194411</td>\n",
       "      <td>0.189189</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.017724</td>\n",
       "      <td>0.804628</td>\n",
       "      <td>0.036465</td>\n",
       "      <td>0.069768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.013300</td>\n",
       "      <td>0.012165</td>\n",
       "      <td>-17.968750</td>\n",
       "      <td>2.707031</td>\n",
       "      <td>-7.824692</td>\n",
       "      <td>4</td>\n",
       "      <td>0.003106</td>\n",
       "      <td>0.192445</td>\n",
       "      <td>0.203069</td>\n",
       "      <td>0.197614</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.022548</td>\n",
       "      <td>0.780940</td>\n",
       "      <td>0.043910</td>\n",
       "      <td>0.083144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.013100</td>\n",
       "      <td>0.012078</td>\n",
       "      <td>-18.078125</td>\n",
       "      <td>2.738281</td>\n",
       "      <td>-7.945001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.003064</td>\n",
       "      <td>0.201938</td>\n",
       "      <td>0.213086</td>\n",
       "      <td>0.207362</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.021223</td>\n",
       "      <td>0.800391</td>\n",
       "      <td>0.040255</td>\n",
       "      <td>0.076655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.013000</td>\n",
       "      <td>0.011819</td>\n",
       "      <td>-18.046875</td>\n",
       "      <td>2.814453</td>\n",
       "      <td>-7.898041</td>\n",
       "      <td>4</td>\n",
       "      <td>0.003189</td>\n",
       "      <td>0.207461</td>\n",
       "      <td>0.218914</td>\n",
       "      <td>0.213034</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.022589</td>\n",
       "      <td>0.783979</td>\n",
       "      <td>0.045117</td>\n",
       "      <td>0.085323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.012900</td>\n",
       "      <td>0.011698</td>\n",
       "      <td>-18.031250</td>\n",
       "      <td>3.070312</td>\n",
       "      <td>-7.979887</td>\n",
       "      <td>4</td>\n",
       "      <td>0.003292</td>\n",
       "      <td>0.211825</td>\n",
       "      <td>0.223519</td>\n",
       "      <td>0.217515</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.022589</td>\n",
       "      <td>0.761126</td>\n",
       "      <td>0.045772</td>\n",
       "      <td>0.086351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.012600</td>\n",
       "      <td>0.011638</td>\n",
       "      <td>-18.843750</td>\n",
       "      <td>3.101562</td>\n",
       "      <td>-8.151798</td>\n",
       "      <td>4</td>\n",
       "      <td>0.003789</td>\n",
       "      <td>0.214874</td>\n",
       "      <td>0.226736</td>\n",
       "      <td>0.220645</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.022776</td>\n",
       "      <td>0.744568</td>\n",
       "      <td>0.048290</td>\n",
       "      <td>0.090698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.011412</td>\n",
       "      <td>-18.921875</td>\n",
       "      <td>3.427734</td>\n",
       "      <td>-7.921619</td>\n",
       "      <td>4</td>\n",
       "      <td>0.003748</td>\n",
       "      <td>0.220040</td>\n",
       "      <td>0.232187</td>\n",
       "      <td>0.225950</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.021823</td>\n",
       "      <td>0.757797</td>\n",
       "      <td>0.051627</td>\n",
       "      <td>0.096669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.012300</td>\n",
       "      <td>0.011342</td>\n",
       "      <td>-19.187500</td>\n",
       "      <td>3.404297</td>\n",
       "      <td>-8.136908</td>\n",
       "      <td>4</td>\n",
       "      <td>0.003706</td>\n",
       "      <td>0.224652</td>\n",
       "      <td>0.237054</td>\n",
       "      <td>0.230686</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.022734</td>\n",
       "      <td>0.737695</td>\n",
       "      <td>0.054686</td>\n",
       "      <td>0.101824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.012100</td>\n",
       "      <td>0.011249</td>\n",
       "      <td>-19.421875</td>\n",
       "      <td>3.525391</td>\n",
       "      <td>-8.140358</td>\n",
       "      <td>4</td>\n",
       "      <td>0.003623</td>\n",
       "      <td>0.227172</td>\n",
       "      <td>0.239714</td>\n",
       "      <td>0.233275</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.021658</td>\n",
       "      <td>0.789719</td>\n",
       "      <td>0.048083</td>\n",
       "      <td>0.090646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.012200</td>\n",
       "      <td>0.011182</td>\n",
       "      <td>-19.968750</td>\n",
       "      <td>3.794922</td>\n",
       "      <td>-8.206929</td>\n",
       "      <td>4</td>\n",
       "      <td>0.004058</td>\n",
       "      <td>0.230418</td>\n",
       "      <td>0.243138</td>\n",
       "      <td>0.236607</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.028263</td>\n",
       "      <td>0.757458</td>\n",
       "      <td>0.058663</td>\n",
       "      <td>0.108892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.012100</td>\n",
       "      <td>0.011082</td>\n",
       "      <td>-20.125000</td>\n",
       "      <td>3.876953</td>\n",
       "      <td>-8.221990</td>\n",
       "      <td>4</td>\n",
       "      <td>0.003913</td>\n",
       "      <td>0.233353</td>\n",
       "      <td>0.246235</td>\n",
       "      <td>0.239621</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.028242</td>\n",
       "      <td>0.762519</td>\n",
       "      <td>0.059050</td>\n",
       "      <td>0.109612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.011800</td>\n",
       "      <td>0.011050</td>\n",
       "      <td>-19.625000</td>\n",
       "      <td>3.861328</td>\n",
       "      <td>-8.390240</td>\n",
       "      <td>4</td>\n",
       "      <td>0.003955</td>\n",
       "      <td>0.236159</td>\n",
       "      <td>0.249196</td>\n",
       "      <td>0.242502</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.029588</td>\n",
       "      <td>0.753455</td>\n",
       "      <td>0.063131</td>\n",
       "      <td>0.116500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.011800</td>\n",
       "      <td>0.010968</td>\n",
       "      <td>-20.359375</td>\n",
       "      <td>3.808594</td>\n",
       "      <td>-8.476894</td>\n",
       "      <td>4</td>\n",
       "      <td>0.004617</td>\n",
       "      <td>0.239078</td>\n",
       "      <td>0.252276</td>\n",
       "      <td>0.245500</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.032072</td>\n",
       "      <td>0.745928</td>\n",
       "      <td>0.064529</td>\n",
       "      <td>0.118782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>0.010887</td>\n",
       "      <td>-20.562500</td>\n",
       "      <td>4.007812</td>\n",
       "      <td>-8.431936</td>\n",
       "      <td>4</td>\n",
       "      <td>0.004804</td>\n",
       "      <td>0.241195</td>\n",
       "      <td>0.254510</td>\n",
       "      <td>0.247674</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.034350</td>\n",
       "      <td>0.742435</td>\n",
       "      <td>0.069685</td>\n",
       "      <td>0.127411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.011500</td>\n",
       "      <td>0.010816</td>\n",
       "      <td>-21.109375</td>\n",
       "      <td>3.896484</td>\n",
       "      <td>-8.405333</td>\n",
       "      <td>4</td>\n",
       "      <td>0.004928</td>\n",
       "      <td>0.243074</td>\n",
       "      <td>0.256493</td>\n",
       "      <td>0.249603</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.033832</td>\n",
       "      <td>0.765816</td>\n",
       "      <td>0.064928</td>\n",
       "      <td>0.119706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.011600</td>\n",
       "      <td>0.010741</td>\n",
       "      <td>-21.718750</td>\n",
       "      <td>4.046875</td>\n",
       "      <td>-8.441212</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005031</td>\n",
       "      <td>0.246009</td>\n",
       "      <td>0.259590</td>\n",
       "      <td>0.252617</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.033770</td>\n",
       "      <td>0.749216</td>\n",
       "      <td>0.071815</td>\n",
       "      <td>0.131067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>0.010712</td>\n",
       "      <td>-21.953125</td>\n",
       "      <td>4.207031</td>\n",
       "      <td>-8.513378</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005238</td>\n",
       "      <td>0.248049</td>\n",
       "      <td>0.261742</td>\n",
       "      <td>0.254711</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.033315</td>\n",
       "      <td>0.752349</td>\n",
       "      <td>0.070422</td>\n",
       "      <td>0.128790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>0.010650</td>\n",
       "      <td>-22.765625</td>\n",
       "      <td>4.203125</td>\n",
       "      <td>-8.541701</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005301</td>\n",
       "      <td>0.249368</td>\n",
       "      <td>0.263135</td>\n",
       "      <td>0.256067</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.035013</td>\n",
       "      <td>0.738541</td>\n",
       "      <td>0.076125</td>\n",
       "      <td>0.138023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>0.010574</td>\n",
       "      <td>-22.937500</td>\n",
       "      <td>4.132812</td>\n",
       "      <td>-8.480398</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005176</td>\n",
       "      <td>0.251061</td>\n",
       "      <td>0.264921</td>\n",
       "      <td>0.257805</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.039319</td>\n",
       "      <td>0.735449</td>\n",
       "      <td>0.080615</td>\n",
       "      <td>0.145302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>0.010562</td>\n",
       "      <td>-23.250000</td>\n",
       "      <td>4.359375</td>\n",
       "      <td>-8.557292</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005259</td>\n",
       "      <td>0.252133</td>\n",
       "      <td>0.266052</td>\n",
       "      <td>0.258905</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.037953</td>\n",
       "      <td>0.746231</td>\n",
       "      <td>0.078413</td>\n",
       "      <td>0.141915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.011200</td>\n",
       "      <td>0.010514</td>\n",
       "      <td>-23.609375</td>\n",
       "      <td>4.320312</td>\n",
       "      <td>-8.671599</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005404</td>\n",
       "      <td>0.253598</td>\n",
       "      <td>0.267597</td>\n",
       "      <td>0.260409</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.039547</td>\n",
       "      <td>0.747437</td>\n",
       "      <td>0.081631</td>\n",
       "      <td>0.147186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.011200</td>\n",
       "      <td>0.010474</td>\n",
       "      <td>-24.296875</td>\n",
       "      <td>4.378906</td>\n",
       "      <td>-8.618743</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005487</td>\n",
       "      <td>0.253975</td>\n",
       "      <td>0.267996</td>\n",
       "      <td>0.260797</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.039795</td>\n",
       "      <td>0.753837</td>\n",
       "      <td>0.082095</td>\n",
       "      <td>0.148065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>0.010431</td>\n",
       "      <td>-25.296875</td>\n",
       "      <td>4.363281</td>\n",
       "      <td>-8.708680</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005280</td>\n",
       "      <td>0.255730</td>\n",
       "      <td>0.269848</td>\n",
       "      <td>0.262599</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.039402</td>\n",
       "      <td>0.757548</td>\n",
       "      <td>0.081816</td>\n",
       "      <td>0.147683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.011100</td>\n",
       "      <td>0.010406</td>\n",
       "      <td>-25.515625</td>\n",
       "      <td>4.484375</td>\n",
       "      <td>-8.770773</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005404</td>\n",
       "      <td>0.256403</td>\n",
       "      <td>0.270558</td>\n",
       "      <td>0.263290</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.041328</td>\n",
       "      <td>0.747829</td>\n",
       "      <td>0.084181</td>\n",
       "      <td>0.151328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.010900</td>\n",
       "      <td>0.010356</td>\n",
       "      <td>-25.843750</td>\n",
       "      <td>4.906250</td>\n",
       "      <td>-8.764424</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005280</td>\n",
       "      <td>0.258028</td>\n",
       "      <td>0.272273</td>\n",
       "      <td>0.264959</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.040437</td>\n",
       "      <td>0.747832</td>\n",
       "      <td>0.084799</td>\n",
       "      <td>0.152325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>0.011000</td>\n",
       "      <td>0.010337</td>\n",
       "      <td>-26.656250</td>\n",
       "      <td>4.683594</td>\n",
       "      <td>-8.762877</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005425</td>\n",
       "      <td>0.257283</td>\n",
       "      <td>0.271486</td>\n",
       "      <td>0.264194</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.040686</td>\n",
       "      <td>0.749666</td>\n",
       "      <td>0.085924</td>\n",
       "      <td>0.154176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>0.010800</td>\n",
       "      <td>0.010333</td>\n",
       "      <td>-27.171875</td>\n",
       "      <td>4.652344</td>\n",
       "      <td>-8.836324</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005383</td>\n",
       "      <td>0.259048</td>\n",
       "      <td>0.273349</td>\n",
       "      <td>0.266007</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.040375</td>\n",
       "      <td>0.763128</td>\n",
       "      <td>0.084061</td>\n",
       "      <td>0.151441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>0.010900</td>\n",
       "      <td>0.010295</td>\n",
       "      <td>-27.640625</td>\n",
       "      <td>4.765625</td>\n",
       "      <td>-8.950812</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005653</td>\n",
       "      <td>0.260632</td>\n",
       "      <td>0.275020</td>\n",
       "      <td>0.267633</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.041017</td>\n",
       "      <td>0.754467</td>\n",
       "      <td>0.087879</td>\n",
       "      <td>0.157422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>0.010900</td>\n",
       "      <td>0.010221</td>\n",
       "      <td>-27.625000</td>\n",
       "      <td>4.941406</td>\n",
       "      <td>-8.852627</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005715</td>\n",
       "      <td>0.261988</td>\n",
       "      <td>0.276451</td>\n",
       "      <td>0.269026</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.042135</td>\n",
       "      <td>0.751530</td>\n",
       "      <td>0.089212</td>\n",
       "      <td>0.159491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>0.010207</td>\n",
       "      <td>-27.671875</td>\n",
       "      <td>5.011719</td>\n",
       "      <td>-8.856679</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005756</td>\n",
       "      <td>0.263298</td>\n",
       "      <td>0.277833</td>\n",
       "      <td>0.270370</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.042818</td>\n",
       "      <td>0.749665</td>\n",
       "      <td>0.088524</td>\n",
       "      <td>0.158349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82000</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>0.010192</td>\n",
       "      <td>-28.234375</td>\n",
       "      <td>4.878906</td>\n",
       "      <td>-8.910546</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005777</td>\n",
       "      <td>0.262987</td>\n",
       "      <td>0.277506</td>\n",
       "      <td>0.270051</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.040334</td>\n",
       "      <td>0.733641</td>\n",
       "      <td>0.092161</td>\n",
       "      <td>0.163752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84000</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>0.010154</td>\n",
       "      <td>-28.343750</td>\n",
       "      <td>5.042969</td>\n",
       "      <td>-8.988853</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005860</td>\n",
       "      <td>0.265312</td>\n",
       "      <td>0.279958</td>\n",
       "      <td>0.272438</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.042881</td>\n",
       "      <td>0.760669</td>\n",
       "      <td>0.087721</td>\n",
       "      <td>0.157301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86000</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>0.010154</td>\n",
       "      <td>-28.671875</td>\n",
       "      <td>4.976562</td>\n",
       "      <td>-8.962641</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005694</td>\n",
       "      <td>0.265332</td>\n",
       "      <td>0.279980</td>\n",
       "      <td>0.272459</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.042943</td>\n",
       "      <td>0.757999</td>\n",
       "      <td>0.091615</td>\n",
       "      <td>0.163472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88000</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>0.010094</td>\n",
       "      <td>-29.578125</td>\n",
       "      <td>4.863281</td>\n",
       "      <td>-8.998302</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005797</td>\n",
       "      <td>0.266844</td>\n",
       "      <td>0.281575</td>\n",
       "      <td>0.274011</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.043792</td>\n",
       "      <td>0.745149</td>\n",
       "      <td>0.094608</td>\n",
       "      <td>0.167899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>0.010600</td>\n",
       "      <td>0.010077</td>\n",
       "      <td>-30.125000</td>\n",
       "      <td>5.058594</td>\n",
       "      <td>-9.070150</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005797</td>\n",
       "      <td>0.268107</td>\n",
       "      <td>0.282908</td>\n",
       "      <td>0.275308</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.042736</td>\n",
       "      <td>0.758055</td>\n",
       "      <td>0.092910</td>\n",
       "      <td>0.165531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92000</td>\n",
       "      <td>0.010600</td>\n",
       "      <td>0.010065</td>\n",
       "      <td>-30.187500</td>\n",
       "      <td>5.292969</td>\n",
       "      <td>-9.117162</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005611</td>\n",
       "      <td>0.268190</td>\n",
       "      <td>0.282995</td>\n",
       "      <td>0.275393</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.043150</td>\n",
       "      <td>0.747942</td>\n",
       "      <td>0.095788</td>\n",
       "      <td>0.169827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94000</td>\n",
       "      <td>0.010800</td>\n",
       "      <td>0.010040</td>\n",
       "      <td>-30.640625</td>\n",
       "      <td>5.207031</td>\n",
       "      <td>-9.057384</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006025</td>\n",
       "      <td>0.268971</td>\n",
       "      <td>0.283820</td>\n",
       "      <td>0.276196</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.042901</td>\n",
       "      <td>0.743788</td>\n",
       "      <td>0.097121</td>\n",
       "      <td>0.171808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96000</td>\n",
       "      <td>0.010500</td>\n",
       "      <td>0.009989</td>\n",
       "      <td>-31.140625</td>\n",
       "      <td>5.222656</td>\n",
       "      <td>-9.000147</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006377</td>\n",
       "      <td>0.270457</td>\n",
       "      <td>0.285387</td>\n",
       "      <td>0.277722</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.043191</td>\n",
       "      <td>0.765111</td>\n",
       "      <td>0.093336</td>\n",
       "      <td>0.166375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98000</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>0.010022</td>\n",
       "      <td>-31.312500</td>\n",
       "      <td>5.449219</td>\n",
       "      <td>-9.159096</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005901</td>\n",
       "      <td>0.268956</td>\n",
       "      <td>0.283803</td>\n",
       "      <td>0.276180</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.042529</td>\n",
       "      <td>0.759731</td>\n",
       "      <td>0.094887</td>\n",
       "      <td>0.168704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>0.009990</td>\n",
       "      <td>-31.828125</td>\n",
       "      <td>5.460938</td>\n",
       "      <td>-9.135222</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006025</td>\n",
       "      <td>0.270477</td>\n",
       "      <td>0.285409</td>\n",
       "      <td>0.277743</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.042653</td>\n",
       "      <td>0.748399</td>\n",
       "      <td>0.098328</td>\n",
       "      <td>0.173819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102000</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>0.009983</td>\n",
       "      <td>-32.250000</td>\n",
       "      <td>5.449219</td>\n",
       "      <td>-9.195067</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006522</td>\n",
       "      <td>0.270964</td>\n",
       "      <td>0.285923</td>\n",
       "      <td>0.278242</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.042715</td>\n",
       "      <td>0.751257</td>\n",
       "      <td>0.097924</td>\n",
       "      <td>0.173263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104000</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>0.009940</td>\n",
       "      <td>-32.625000</td>\n",
       "      <td>5.527344</td>\n",
       "      <td>-9.149701</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006274</td>\n",
       "      <td>0.272124</td>\n",
       "      <td>0.287146</td>\n",
       "      <td>0.279433</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.044226</td>\n",
       "      <td>0.740918</td>\n",
       "      <td>0.102048</td>\n",
       "      <td>0.179388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106000</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>0.009921</td>\n",
       "      <td>-32.843750</td>\n",
       "      <td>5.460938</td>\n",
       "      <td>-9.197285</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006108</td>\n",
       "      <td>0.272413</td>\n",
       "      <td>0.287452</td>\n",
       "      <td>0.279731</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.043564</td>\n",
       "      <td>0.753734</td>\n",
       "      <td>0.098951</td>\n",
       "      <td>0.174936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108000</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>0.009956</td>\n",
       "      <td>-32.718750</td>\n",
       "      <td>5.457031</td>\n",
       "      <td>-9.344961</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006605</td>\n",
       "      <td>0.273330</td>\n",
       "      <td>0.288419</td>\n",
       "      <td>0.280672</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.044309</td>\n",
       "      <td>0.756531</td>\n",
       "      <td>0.098541</td>\n",
       "      <td>0.174370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110000</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>0.009881</td>\n",
       "      <td>-33.125000</td>\n",
       "      <td>5.660156</td>\n",
       "      <td>-9.240895</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006853</td>\n",
       "      <td>0.274789</td>\n",
       "      <td>0.289959</td>\n",
       "      <td>0.282170</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.045220</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>0.105527</td>\n",
       "      <td>0.184773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112000</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>0.009897</td>\n",
       "      <td>-33.562500</td>\n",
       "      <td>5.738281</td>\n",
       "      <td>-9.310121</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006895</td>\n",
       "      <td>0.274681</td>\n",
       "      <td>0.289844</td>\n",
       "      <td>0.282059</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.045655</td>\n",
       "      <td>0.760297</td>\n",
       "      <td>0.100518</td>\n",
       "      <td>0.177561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114000</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>0.009866</td>\n",
       "      <td>-34.000000</td>\n",
       "      <td>5.335938</td>\n",
       "      <td>-9.295578</td>\n",
       "      <td>4</td>\n",
       "      <td>0.007019</td>\n",
       "      <td>0.275612</td>\n",
       "      <td>0.290828</td>\n",
       "      <td>0.283016</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.045075</td>\n",
       "      <td>0.736963</td>\n",
       "      <td>0.107138</td>\n",
       "      <td>0.187079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.009842</td>\n",
       "      <td>-34.125000</td>\n",
       "      <td>5.835938</td>\n",
       "      <td>-9.334861</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006750</td>\n",
       "      <td>0.277124</td>\n",
       "      <td>0.292422</td>\n",
       "      <td>0.284568</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.044351</td>\n",
       "      <td>0.749151</td>\n",
       "      <td>0.104790</td>\n",
       "      <td>0.183861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118000</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>0.009846</td>\n",
       "      <td>-34.468750</td>\n",
       "      <td>5.824219</td>\n",
       "      <td>-9.333791</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006957</td>\n",
       "      <td>0.276161</td>\n",
       "      <td>0.291407</td>\n",
       "      <td>0.283579</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.044744</td>\n",
       "      <td>0.746680</td>\n",
       "      <td>0.104730</td>\n",
       "      <td>0.183694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120000</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>0.009811</td>\n",
       "      <td>-34.343750</td>\n",
       "      <td>5.933594</td>\n",
       "      <td>-9.328420</td>\n",
       "      <td>4</td>\n",
       "      <td>0.007019</td>\n",
       "      <td>0.277926</td>\n",
       "      <td>0.293269</td>\n",
       "      <td>0.285392</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.045862</td>\n",
       "      <td>0.748849</td>\n",
       "      <td>0.107488</td>\n",
       "      <td>0.187992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122000</td>\n",
       "      <td>0.010100</td>\n",
       "      <td>0.009827</td>\n",
       "      <td>-35.406250</td>\n",
       "      <td>5.484375</td>\n",
       "      <td>-9.432419</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006750</td>\n",
       "      <td>0.277693</td>\n",
       "      <td>0.293023</td>\n",
       "      <td>0.285152</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.045013</td>\n",
       "      <td>0.750462</td>\n",
       "      <td>0.106576</td>\n",
       "      <td>0.186645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124000</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>0.009819</td>\n",
       "      <td>-35.406250</td>\n",
       "      <td>5.695312</td>\n",
       "      <td>-9.386559</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006874</td>\n",
       "      <td>0.278998</td>\n",
       "      <td>0.294400</td>\n",
       "      <td>0.286492</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.044268</td>\n",
       "      <td>0.746867</td>\n",
       "      <td>0.107734</td>\n",
       "      <td>0.188305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126000</td>\n",
       "      <td>0.010100</td>\n",
       "      <td>0.009793</td>\n",
       "      <td>-35.750000</td>\n",
       "      <td>5.734375</td>\n",
       "      <td>-9.426833</td>\n",
       "      <td>4</td>\n",
       "      <td>0.007019</td>\n",
       "      <td>0.279401</td>\n",
       "      <td>0.294826</td>\n",
       "      <td>0.286906</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.046421</td>\n",
       "      <td>0.755685</td>\n",
       "      <td>0.106554</td>\n",
       "      <td>0.186772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.009794</td>\n",
       "      <td>-35.812500</td>\n",
       "      <td>6.101562</td>\n",
       "      <td>-9.438511</td>\n",
       "      <td>4</td>\n",
       "      <td>0.007268</td>\n",
       "      <td>0.280038</td>\n",
       "      <td>0.295498</td>\n",
       "      <td>0.287560</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.046939</td>\n",
       "      <td>0.741800</td>\n",
       "      <td>0.110552</td>\n",
       "      <td>0.192427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.009756</td>\n",
       "      <td>-35.875000</td>\n",
       "      <td>6.042969</td>\n",
       "      <td>-9.379709</td>\n",
       "      <td>4</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>0.280773</td>\n",
       "      <td>0.296273</td>\n",
       "      <td>0.288315</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.046463</td>\n",
       "      <td>0.763026</td>\n",
       "      <td>0.106226</td>\n",
       "      <td>0.186490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.009758</td>\n",
       "      <td>-36.125000</td>\n",
       "      <td>6.101562</td>\n",
       "      <td>-9.514573</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006957</td>\n",
       "      <td>0.280897</td>\n",
       "      <td>0.296404</td>\n",
       "      <td>0.288443</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.046111</td>\n",
       "      <td>0.749618</td>\n",
       "      <td>0.109957</td>\n",
       "      <td>0.191782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134000</td>\n",
       "      <td>0.010100</td>\n",
       "      <td>0.009743</td>\n",
       "      <td>-36.375000</td>\n",
       "      <td>6.097656</td>\n",
       "      <td>-9.449801</td>\n",
       "      <td>4</td>\n",
       "      <td>0.007247</td>\n",
       "      <td>0.281803</td>\n",
       "      <td>0.297360</td>\n",
       "      <td>0.289373</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.046918</td>\n",
       "      <td>0.747590</td>\n",
       "      <td>0.111382</td>\n",
       "      <td>0.193879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136000</td>\n",
       "      <td>0.009800</td>\n",
       "      <td>0.009724</td>\n",
       "      <td>-36.312500</td>\n",
       "      <td>5.679688</td>\n",
       "      <td>-9.493155</td>\n",
       "      <td>4</td>\n",
       "      <td>0.007164</td>\n",
       "      <td>0.281860</td>\n",
       "      <td>0.297420</td>\n",
       "      <td>0.289431</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.047022</td>\n",
       "      <td>0.759530</td>\n",
       "      <td>0.110137</td>\n",
       "      <td>0.192378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138000</td>\n",
       "      <td>0.009900</td>\n",
       "      <td>0.009724</td>\n",
       "      <td>-36.625000</td>\n",
       "      <td>5.742188</td>\n",
       "      <td>-9.567405</td>\n",
       "      <td>4</td>\n",
       "      <td>0.007268</td>\n",
       "      <td>0.282554</td>\n",
       "      <td>0.298152</td>\n",
       "      <td>0.290143</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.046959</td>\n",
       "      <td>0.750294</td>\n",
       "      <td>0.111437</td>\n",
       "      <td>0.194052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.009706</td>\n",
       "      <td>-37.187500</td>\n",
       "      <td>5.859375</td>\n",
       "      <td>-9.513268</td>\n",
       "      <td>4</td>\n",
       "      <td>0.007247</td>\n",
       "      <td>0.282036</td>\n",
       "      <td>0.297606</td>\n",
       "      <td>0.289612</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.046752</td>\n",
       "      <td>0.744156</td>\n",
       "      <td>0.112497</td>\n",
       "      <td>0.195447</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a4f66a-b07c-46c7-bba8-f26d88542aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test set\n",
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1f6868-6ca4-4387-a882-ff56b5172897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred, top_k_range=(1, 5), threshold_range=(0.025, 0.5, 0.025)):\n",
    "    logits, labels = pred\n",
    "\n",
    "    # Calculate logit statistics\n",
    "    logit_min = logits.min()\n",
    "    logit_max = logits.max()\n",
    "    logit_mean = logits.mean()\n",
    "\n",
    "    def top_k_pred(logits, k):\n",
    "        top_k_indices = np.argsort(-logits, axis=1)[:, :k]\n",
    "        predictions = np.zeros_like(logits, dtype=int)\n",
    "        for i, indices in enumerate(top_k_indices):\n",
    "            predictions[i, indices] = 1\n",
    "        return predictions\n",
    "\n",
    "    def logit_threshold_pred(logits, threshold):\n",
    "        return (logits > threshold).astype(int)\n",
    "\n",
    "    def calc_metrics(labels, predictions):\n",
    "        subset_accuracy = accuracy_score(labels, predictions)\n",
    "        precision = precision_score(labels, predictions, average=\"micro\")\n",
    "        recall = recall_score(labels, predictions, average=\"micro\")\n",
    "        f1 = f1_score(labels, predictions, average=\"micro\")\n",
    "        return subset_accuracy, precision, recall, f1\n",
    "\n",
    "    # Store results for top-k\n",
    "    top_k_results = {}\n",
    "    for k in range(*top_k_range):\n",
    "        predictions = top_k_pred(logits, k)\n",
    "        subset_accuracy, precision, recall, f1 = calc_metrics(labels, predictions)\n",
    "        top_k_results[k] = {\n",
    "            \"accuracy\": subset_accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1\n",
    "        }\n",
    "\n",
    "    # Find optimal k\n",
    "    optimal_k = max(top_k_results, key=lambda k: top_k_results[k][\"f1\"])\n",
    "    optimal_k_metrics = top_k_results[optimal_k]\n",
    "\n",
    "    # Store results for thresholds\n",
    "    threshold_results = {}\n",
    "    for threshold in np.arange(*threshold_range):\n",
    "        predictions = logit_threshold_pred(logits, threshold)\n",
    "        subset_accuracy, precision, recall, f1 = calc_metrics(labels, predictions)\n",
    "        threshold_results[threshold] = {\n",
    "            \"accuracy\": subset_accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1\n",
    "        }\n",
    "\n",
    "    # Find optimal threshold\n",
    "    optimal_threshold = max(threshold_results, key=lambda t: threshold_results[t][\"f1\"])\n",
    "    optimal_threshold_metrics = threshold_results[optimal_threshold]\n",
    "\n",
    "    # Return detailed metrics\n",
    "    return {\n",
    "        \"logit_min\": logit_min,\n",
    "        \"logit_max\": logit_max,\n",
    "        \"logit_mean\": logit_mean,\n",
    "        \"optimal_k\": optimal_k,\n",
    "        \"optimal_k_accuracy\": optimal_k_metrics[\"accuracy\"],\n",
    "        \"optimal_k_precision\": optimal_k_metrics[\"precision\"],\n",
    "        \"optimal_k_recall\": optimal_k_metrics[\"recall\"],\n",
    "        \"optimal_k_f1\": optimal_k_metrics[\"f1\"],\n",
    "        \"optimal_threshold\": optimal_threshold,\n",
    "        \"optimal_threshold_accuracy\": optimal_threshold_metrics[\"accuracy\"],\n",
    "        \"optimal_threshold_precision\": optimal_threshold_metrics[\"precision\"],\n",
    "        \"optimal_threshold_recall\": optimal_threshold_metrics[\"recall\"],\n",
    "        \"optimal_threshold_f1\": optimal_threshold_metrics[\"f1\"]\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "num_labels = len(y_train[0])\n",
    "model = MultiLabelRoBERTa(num_labels=num_labels)\n",
    "model.roberta.gradient_checkpointing_enable()\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=4000,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=4000,\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    # gradient_accumulation_steps=2,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True\n",
    ")\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f187b57-4f24-4b56-857a-e6dcfed62751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7962725-e355-49f4-a9f7-3335282eb031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test set\n",
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd4e3eb-e5c6-43a0-8103-b3d4c7d54ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterSampler\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Define hyperparameter search space\n",
    "param_grid = {\n",
    "    \"learning_rate\": [1e-3, 1e-4, 1e-5, 5e-5],\n",
    "    \"num_train_epochs\": [5, 10, 20],\n",
    "    \"per_device_train_batch_size\": [16, 32, 64],\n",
    "    \"weight_decay\": [0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "# Generate random samples of hyperparameters\n",
    "n_trials = 10  # Number of trials to sample\n",
    "param_samples = list(ParameterSampler(param_grid, n_iter=n_trials, random_state=478))\n",
    "\n",
    "best_score = -np.inf\n",
    "best_params = None\n",
    "best_model = None\n",
    "\n",
    "# Iterate over sampled hyperparameters\n",
    "for i, params in enumerate(param_samples):\n",
    "    print(f\"Trial {i+1}/{n_trials} with parameters: {params}\")\n",
    "\n",
    "    # Update TrainingArguments dynamically\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results/trial_{i}\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=4000,\n",
    "        save_strategy=\"no\",\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        per_device_train_batch_size=params[\"per_device_train_batch_size\"],\n",
    "        num_train_epochs=params[\"num_train_epochs\"],\n",
    "        weight_decay=params[\"weight_decay\"],\n",
    "        fp16=True  # Mixed precision for speed\n",
    "    )\n",
    "\n",
    "    # Define the Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=dev_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate on the validation set\n",
    "    metrics = trainer.evaluate(dev_dataset)\n",
    "    f1 = metrics[\"eval_f1\"]  # Adjust based on your compute_metrics keys\n",
    "\n",
    "    print(f\"Trial {i+1} F1 Score: {f1}\")\n",
    "\n",
    "    # Keep track of the best model\n",
    "    if f1 > best_score:\n",
    "        best_score = f1\n",
    "        best_params = params\n",
    "        best_model = trainer.model  # Save the best model\n",
    "\n",
    "# Output the best results\n",
    "print(f\"Best F1 Score: {best_score}\")\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "# Save the best model\n",
    "best_model.save_pretrained(\"./best_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77f6a58-ca72-44c8-a214-1e2d991b52ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
