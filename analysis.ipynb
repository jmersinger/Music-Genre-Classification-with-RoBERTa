{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a72bd69-a821-4a47-af97-b1ad2170fc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Allocated GPU memory: 0.00 GB\n",
      "Reserved GPU memory: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Optionally allocate a fraction of GPU memory\n",
    "torch.cuda.set_per_process_memory_fraction(0.9, 0)\n",
    "\n",
    "# Check memory stats\n",
    "print(f'Allocated GPU memory: {torch.cuda.memory_allocated(device) / (1024 ** 3):.2f} GB')\n",
    "print(f'Reserved GPU memory: {torch.cuda.memory_reserved(device) / (1024 ** 3):.2f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "175073ea-4071-4aaf-a332-e354846b474c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.cleaning import download_and_clean\n",
    "\n",
    "artists, tracks = download_and_clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d54a8e77-2193-4fe4-8043-282cc51d056f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         id  followers                      genres  \\\n",
      "45   0VLMVnVbJyJ4oyZs2L3Yl2       71.0          ['carnaval cadiz']   \n",
      "46   0dt23bs4w8zx154C5xdVyl       63.0          ['carnaval cadiz']   \n",
      "47   0pGhoB99qpEJEsBQxgaskQ       64.0          ['carnaval cadiz']   \n",
      "48   3HDrX2OtSuXLW5dLR85uN3       53.0          ['carnaval cadiz']   \n",
      "136  22mLrN5fkppmuUPsHx6i2G       59.0  ['classical harp', 'harp']   \n",
      "\n",
      "                             name  popularity  \n",
      "45   Las Viudas De Los Bisabuelos           6  \n",
      "46              Los De Capuchinos           5  \n",
      "47             Los “Pofesionales”           7  \n",
      "48      Los Que No Paran De Rajar           6  \n",
      "136                   Vera Dulova           3  \n"
     ]
    }
   ],
   "source": [
    "print(artists.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07ae631a-3033-429d-8484-1f0d06c826a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       id                                name  popularity  \\\n",
      "2  07A5yehtSnoedViJAZkNnc  Vivo para Quererte - Remasterizado           0   \n",
      "3  08FmqUhxtyLTn6pAh6bk45       El Prisionero - Remasterizado           0   \n",
      "4  08y9GfoqCWfOGsKdwojr5e                 Lady of the Evening           0   \n",
      "5  0BRXJHRNGQ3W4v9frnSfhu                           Ave Maria           0   \n",
      "7  0IA0Hju8CAgYfV1hwhidBH                             La Java           0   \n",
      "\n",
      "   duration_ms  explicit              artists                  id_artists  \\\n",
      "2       181640         0  ['Ignacio Corsini']  ['5LiOoJbxVSAMkBS2fUm3X2']   \n",
      "3       176907         0  ['Ignacio Corsini']  ['5LiOoJbxVSAMkBS2fUm3X2']   \n",
      "4       163080         0      ['Dick Haymes']  ['3BiJGZsyX9sJchTqcSA7Su']   \n",
      "5       178933         0      ['Dick Haymes']  ['3BiJGZsyX9sJchTqcSA7Su']   \n",
      "7       161427         0      ['Mistinguett']  ['4AxgXfD7ISvJSTObqm4aIE']   \n",
      "\n",
      "  release_date  danceability  energy  ...  loudness  mode  speechiness  \\\n",
      "2   1922-03-21         0.434  0.1770  ...   -21.180     1       0.0512   \n",
      "3   1922-03-21         0.321  0.0946  ...   -27.961     1       0.0504   \n",
      "4         1922         0.402  0.1580  ...   -16.900     0       0.0390   \n",
      "5         1922         0.227  0.2610  ...   -12.343     1       0.0382   \n",
      "7         1922         0.563  0.1840  ...   -13.757     1       0.0512   \n",
      "\n",
      "   acousticness  instrumentalness  liveness  valence    tempo  time_signature  \\\n",
      "2         0.994          0.021800    0.2120   0.4570  130.418               5   \n",
      "3         0.995          0.918000    0.1040   0.3970  169.980               3   \n",
      "4         0.989          0.130000    0.3110   0.1960  103.220               4   \n",
      "5         0.994          0.247000    0.0977   0.0539  118.891               4   \n",
      "7         0.993          0.000016    0.3250   0.6540  133.088               3   \n",
      "\n",
      "                                              genres  \n",
      "2                             [tango, vintage tango]  \n",
      "3                             [tango, vintage tango]  \n",
      "4  [lounge, big band, adult standards, swing, eas...  \n",
      "5  [lounge, big band, adult standards, swing, eas...  \n",
      "7                                  [vintage chanson]  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "print(tracks.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9364f6fa-96a5-40dd-8d47-2a78482f8567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4706"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "all_genres = set(chain.from_iterable(tracks[\"genres\"]))\n",
    "len(all_genres)\n",
    "# all_genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af999b92-4010-4fb9-b5da-6f6cc67380a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_to_index = {genre: idx for idx, genre in enumerate(sorted(all_genres))}\n",
    "# genre_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f8f25fb-1407-42a6-9989-c04e25dd56c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multihot_vector(genres, index_dict):\n",
    "    multihot = [0] * len(index_dict)\n",
    "    for genre in genres:\n",
    "        multihot[index_dict[genre]] = 1\n",
    "    return multihot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "428fca29-7f55-4b3e-9537-e2ef8afb9573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "3         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "4         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "5         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "7         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "                                ...                        \n",
       "586667    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "586668    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "586669    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "586670    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "586671    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "Name: multi_hot_genres, Length: 499064, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracks[\"multi_hot_genres\"] = tracks[\"genres\"].apply(\n",
    "    lambda genres: multihot_vector(genres, genre_to_index)\n",
    ")\n",
    "tracks[\"multi_hot_genres\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d5aedaf-66d2-4faf-aa2e-bbe8c2504aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = tracks[[\"name\", \"popularity\", \"duration_ms\", \"explicit\", \"release_date\", \"danceability\", \"energy\", \"key\", \"loudness\", \"mode\", \"speechiness\", \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\", \"tempo\", \"time_signature\"]]\n",
    "y = tracks[\"multi_hot_genres\"].tolist()\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=478)\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=478)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34f732a6-c0f2-4d27-b749-2fa9ebef55b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 399251\n",
      "Dev set size: 49906\n",
      "Test set size: 49907\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train set size: {len(X_train)}\")\n",
    "print(f\"Dev set size: {len(X_dev)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc59b6ea-b98e-42d5-8a6c-91991e03982c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of labels: 4706\n",
      "Filtered number of labels: 1529\n"
     ]
    }
   ],
   "source": [
    "# Count label frequencies in the training set\n",
    "import numpy as np\n",
    "\n",
    "label_counts = np.sum(y_train, axis=0)\n",
    "\n",
    "# Set a threshold for filtering labels\n",
    "threshold =100\n",
    "selected_labels = np.where(label_counts >= threshold)[0]\n",
    "\n",
    "# Filter labels in train, dev, and test sets\n",
    "def filter_labels(y, selected_labels):\n",
    "    return np.array([[y_sample[i] for i in selected_labels] for y_sample in y])\n",
    "\n",
    "y_train = filter_labels(y_train, selected_labels)\n",
    "y_dev = filter_labels(y_dev, selected_labels)\n",
    "y_test = filter_labels(y_test, selected_labels)\n",
    "\n",
    "# Update genre_to_index mapping\n",
    "filtered_genre_to_index = {genre: idx for idx, genre in enumerate(selected_labels)}\n",
    "index_to_genre = {v: k for k, v in filtered_genre_to_index.items()}\n",
    "\n",
    "# Step 5: Print results for verification\n",
    "print(f\"Original number of labels: {len(label_counts)}\")\n",
    "print(f\"Filtered number of labels: {len(selected_labels)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9aa7530-f23d-4471-ac80-0e9c56d18d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size after filtering: 386974\n",
      "Dev set size after filtering: 48297\n",
      "Test set size after filtering: 48376\n"
     ]
    }
   ],
   "source": [
    "# Remove rows with no positive labels\n",
    "def remove_empty_labels(X, y):\n",
    "    non_empty_indices = [i for i, labels in enumerate(y) if np.sum(labels) > 0]\n",
    "    X_filtered = X.iloc[non_empty_indices].reset_index(drop=True)\n",
    "    y_filtered = np.array([y[i] for i in non_empty_indices])\n",
    "    return X_filtered, y_filtered\n",
    "\n",
    "# Apply to train, dev, and test sets\n",
    "X_train, y_train = remove_empty_labels(X_train, y_train)\n",
    "X_dev, y_dev = remove_empty_labels(X_dev, y_dev)\n",
    "X_test, y_test = remove_empty_labels(X_test, y_test)\n",
    "\n",
    "# Verify\n",
    "print(f\"Train set size after filtering: {len(X_train)}\")\n",
    "print(f\"Dev set size after filtering: {len(X_dev)}\")\n",
    "print(f\"Test set size after filtering: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f8a9644-ecff-49a1-a508-e5519cc64df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c31961ab-7c5e-4ca1-8ffc-434ae2406c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization function\n",
    "def tokenize_data(X):\n",
    "    return tokenizer(\n",
    "        list(X[\"name\"]),\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=64,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "# Dataset preparation\n",
    "class MultiLabelDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        tokenized = tokenize_data(X)\n",
    "        self.input_ids = tokenized[\"input_ids\"]\n",
    "        self.attention_mask = tokenized[\"attention_mask\"]\n",
    "        self.labels = torch.tensor(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx],\n",
    "            \"labels\": self.labels[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "29b2fa67-2d35-48f0-bfcb-d1178c908f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert splits into datasets\n",
    "train_dataset = MultiLabelDataset(X_train, y_train)\n",
    "dev_dataset = MultiLabelDataset(X_dev, y_dev)\n",
    "test_dataset = MultiLabelDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b6f708e1-5d64-4f86-beca-030d73f16309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "class MultiLabelRoBERTa(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(MultiLabelRoBERTa, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.roberta.config.hidden_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        logits = self.classifier(outputs.pooler_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            criterion = nn.BCEWithLogitsLoss()\n",
    "            loss = criterion(logits, labels.float())\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "# Metrics computation\n",
    "def compute_metrics(pred, threshold_range=(0.0, 0.2, 0.01)):\n",
    "    logits, labels = pred\n",
    "\n",
    "    # Calculate logit statistics\n",
    "    logit_min = logits.min()\n",
    "    logit_max = logits.max()\n",
    "    logit_mean = logits.mean()\n",
    "\n",
    "    def logit_threshold_pred(logits, threshold):\n",
    "        return (logits > threshold).astype(int)\n",
    "\n",
    "    def calc_metrics(labels, predictions):\n",
    "        subset_accuracy = accuracy_score(labels, predictions)\n",
    "        precision = precision_score(labels, predictions, average=\"micro\")\n",
    "        recall = recall_score(labels, predictions, average=\"micro\")\n",
    "        f1 = f1_score(labels, predictions, average=\"micro\")\n",
    "        return subset_accuracy, precision, recall, f1\n",
    "        \n",
    "    # Store results for thresholds\n",
    "    threshold_results = {}\n",
    "    for threshold in np.arange(*threshold_range):\n",
    "        predictions = logit_threshold_pred(logits, threshold)\n",
    "        subset_accuracy, precision, recall, f1 = calc_metrics(labels, predictions)\n",
    "        threshold_results[threshold] = {\n",
    "            \"accuracy\": subset_accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1\n",
    "        }\n",
    "\n",
    "    # Find optimal threshold\n",
    "    optimal_threshold = max(threshold_results, key=lambda t: threshold_results[t][\"f1\"])\n",
    "    optimal_threshold_metrics = threshold_results[optimal_threshold]\n",
    "\n",
    "    # Return detailed metrics\n",
    "    return {\n",
    "        \"logit_min\": logit_min,\n",
    "        \"logit_max\": logit_max,\n",
    "        \"logit_mean\": logit_mean,\n",
    "        \"optimal_threshold\": optimal_threshold,\n",
    "        \"optimal_threshold_accuracy\": optimal_threshold_metrics[\"accuracy\"],\n",
    "        \"optimal_threshold_precision\": optimal_threshold_metrics[\"precision\"],\n",
    "        \"optimal_threshold_recall\": optimal_threshold_metrics[\"recall\"],\n",
    "        \"optimal_threshold_f1\": optimal_threshold_metrics[\"f1\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c667123f-dd36-4247-a228-36607069bf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "num_labels = len(y_train[0])\n",
    "model = MultiLabelRoBERTa(num_labels=num_labels)\n",
    "model.roberta.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "075be418-828e-4c62-bb1a-f5bf0c8c7d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",\n",
    "#     eval_strategy=\"steps\",\n",
    "#     eval_steps=2000,\n",
    "#     save_strategy=\"steps\",\n",
    "#     save_steps=2000,\n",
    "#     learning_rate=1e-5,\n",
    "#     per_device_train_batch_size=16,\n",
    "#     # gradient_accumulation_steps=2,\n",
    "#     per_device_eval_batch_size=16,\n",
    "#     num_train_epochs=10,\n",
    "#     weight_decay=0.01,\n",
    "#     load_best_model_at_end=True,\n",
    "#     fp16=True\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "53111b9e-98ac-42ef-875b-187ddcf9e8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=dev_dataset,\n",
    "#     processing_class=tokenizer,\n",
    "#     compute_metrics=compute_metrics\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c5a70068-67c3-4ef7-a078-4f4977fb1bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the model\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f0a4f66a-b07c-46c7-bba8-f26d88542aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test set\n",
    "# trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2b1f6868-6ca4-4387-a882-ff56b5172897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_metrics(pred, top_k_range=(1, 10), threshold_range=(0.05, 0.5, 0.05)):\n",
    "#     logits, labels = pred\n",
    "\n",
    "#     # Calculate logit statistics\n",
    "#     logit_min = logits.min()\n",
    "#     logit_max = logits.max()\n",
    "#     logit_mean = logits.mean()\n",
    "\n",
    "#     def top_k_pred(logits, k):\n",
    "#         top_k_indices = np.argsort(-logits, axis=1)[:, :k]\n",
    "#         predictions = np.zeros_like(logits, dtype=int)\n",
    "#         for i, indices in enumerate(top_k_indices):\n",
    "#             predictions[i, indices] = 1\n",
    "#         return predictions\n",
    "\n",
    "#     def logit_threshold_pred(logits, threshold):\n",
    "#         return (logits > threshold).astype(int)\n",
    "\n",
    "#     def calc_metrics(labels, predictions):\n",
    "#         subset_accuracy = accuracy_score(labels, predictions)\n",
    "#         precision = precision_score(labels, predictions, average=\"micro\")\n",
    "#         recall = recall_score(labels, predictions, average=\"micro\")\n",
    "#         f1 = f1_score(labels, predictions, average=\"micro\")\n",
    "#         return subset_accuracy, precision, recall, f1\n",
    "\n",
    "#     # Store results for top-k\n",
    "#     top_k_results = {}\n",
    "#     for k in range(*top_k_range):\n",
    "#         predictions = top_k_pred(logits, k)\n",
    "#         subset_accuracy, precision, recall, f1 = calc_metrics(labels, predictions)\n",
    "#         top_k_results[k] = {\n",
    "#             \"accuracy\": subset_accuracy,\n",
    "#             \"precision\": precision,\n",
    "#             \"recall\": recall,\n",
    "#             \"f1\": f1\n",
    "#         }\n",
    "\n",
    "#     # Find optimal k\n",
    "#     optimal_k = max(top_k_results, key=lambda k: top_k_results[k][\"f1\"])\n",
    "#     optimal_k_metrics = top_k_results[optimal_k]\n",
    "\n",
    "#     # Store results for thresholds\n",
    "#     threshold_results = {}\n",
    "#     for threshold in np.arange(*threshold_range):\n",
    "#         predictions = logit_threshold_pred(logits, threshold)\n",
    "#         subset_accuracy, precision, recall, f1 = calc_metrics(labels, predictions)\n",
    "#         threshold_results[threshold] = {\n",
    "#             \"accuracy\": subset_accuracy,\n",
    "#             \"precision\": precision,\n",
    "#             \"recall\": recall,\n",
    "#             \"f1\": f1\n",
    "#         }\n",
    "\n",
    "#     # Find optimal threshold\n",
    "#     optimal_threshold = max(threshold_results, key=lambda t: threshold_results[t][\"f1\"])\n",
    "#     optimal_threshold_metrics = threshold_results[optimal_threshold]\n",
    "\n",
    "#     # Return detailed metrics\n",
    "#     return {\n",
    "#         \"logit_min\": logit_min,\n",
    "#         \"logit_max\": logit_max,\n",
    "#         \"logit_mean\": logit_mean,\n",
    "#         \"optimal_k\": optimal_k,\n",
    "#         \"optimal_k_accuracy\": optimal_k_metrics[\"accuracy\"],\n",
    "#         \"optimal_k_precision\": optimal_k_metrics[\"precision\"],\n",
    "#         \"optimal_k_recall\": optimal_k_metrics[\"recall\"],\n",
    "#         \"optimal_k_f1\": optimal_k_metrics[\"f1\"],\n",
    "#         \"optimal_threshold\": optimal_threshold,\n",
    "#         \"optimal_threshold_accuracy\": optimal_threshold_metrics[\"accuracy\"],\n",
    "#         \"optimal_threshold_precision\": optimal_threshold_metrics[\"precision\"],\n",
    "#         \"optimal_threshold_recall\": optimal_threshold_metrics[\"recall\"],\n",
    "#         \"optimal_threshold_f1\": optimal_threshold_metrics[\"f1\"]\n",
    "#     }\n",
    "\n",
    "\n",
    "\n",
    "# # Initialize the model\n",
    "# num_labels = len(y_train[0])\n",
    "# model = MultiLabelRoBERTa(num_labels=num_labels)\n",
    "# model.roberta.gradient_checkpointing_enable()\n",
    "# # Training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",\n",
    "#     eval_strategy=\"steps\",\n",
    "#     eval_steps=4000,\n",
    "#     save_strategy=\"steps\",\n",
    "#     save_steps=4000,\n",
    "#     learning_rate=1e-5,\n",
    "#     per_device_train_batch_size=32,\n",
    "#     # gradient_accumulation_steps=2,\n",
    "#     per_device_eval_batch_size=16,\n",
    "#     num_train_epochs=20,\n",
    "#     weight_decay=0.01,\n",
    "#     load_best_model_at_end=True,\n",
    "#     fp16=True\n",
    "# )\n",
    "# # Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=dev_dataset,\n",
    "#     processing_class=tokenizer,\n",
    "#     compute_metrics=compute_metrics\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5f187b57-4f24-4b56-857a-e6dcfed62751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c7962725-e355-49f4-a9f7-3335282eb031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test set\n",
    "# trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd4e3eb-e5c6-43a0-8103-b3d4c7d54ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1/5 with parameters: {'weight_decay': 0.01, 'per_device_train_batch_size': 16, 'num_train_epochs': 10, 'learning_rate': 1e-05}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71718' max='241860' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 71718/241860 4:46:42 < 11:20:11, 4.17 it/s, Epoch 2.97/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Logit Min</th>\n",
       "      <th>Logit Max</th>\n",
       "      <th>Logit Mean</th>\n",
       "      <th>Optimal Threshold</th>\n",
       "      <th>Optimal Threshold Accuracy</th>\n",
       "      <th>Optimal Threshold Precision</th>\n",
       "      <th>Optimal Threshold Recall</th>\n",
       "      <th>Optimal Threshold F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.016400</td>\n",
       "      <td>0.014273</td>\n",
       "      <td>-13.414062</td>\n",
       "      <td>-0.962402</td>\n",
       "      <td>-7.347102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.013480</td>\n",
       "      <td>-14.125000</td>\n",
       "      <td>0.757324</td>\n",
       "      <td>-7.657314</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017372</td>\n",
       "      <td>0.833412</td>\n",
       "      <td>0.019237</td>\n",
       "      <td>0.037607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.014100</td>\n",
       "      <td>0.012693</td>\n",
       "      <td>-15.648438</td>\n",
       "      <td>2.136719</td>\n",
       "      <td>-7.655869</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017351</td>\n",
       "      <td>0.799750</td>\n",
       "      <td>0.034924</td>\n",
       "      <td>0.066926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.013300</td>\n",
       "      <td>0.012257</td>\n",
       "      <td>-16.812500</td>\n",
       "      <td>2.412109</td>\n",
       "      <td>-7.838567</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017185</td>\n",
       "      <td>0.797413</td>\n",
       "      <td>0.036033</td>\n",
       "      <td>0.068951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.013000</td>\n",
       "      <td>0.011938</td>\n",
       "      <td>-17.609375</td>\n",
       "      <td>2.673828</td>\n",
       "      <td>-8.003518</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021264</td>\n",
       "      <td>0.767641</td>\n",
       "      <td>0.043019</td>\n",
       "      <td>0.081473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.012600</td>\n",
       "      <td>0.011553</td>\n",
       "      <td>-17.890625</td>\n",
       "      <td>2.900391</td>\n",
       "      <td>-7.983685</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022734</td>\n",
       "      <td>0.738987</td>\n",
       "      <td>0.056166</td>\n",
       "      <td>0.104398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.012300</td>\n",
       "      <td>0.011332</td>\n",
       "      <td>-18.625000</td>\n",
       "      <td>3.103516</td>\n",
       "      <td>-8.156084</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022962</td>\n",
       "      <td>0.755815</td>\n",
       "      <td>0.054845</td>\n",
       "      <td>0.102268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.012100</td>\n",
       "      <td>0.011165</td>\n",
       "      <td>-20.218750</td>\n",
       "      <td>3.259766</td>\n",
       "      <td>-8.350224</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027517</td>\n",
       "      <td>0.757324</td>\n",
       "      <td>0.061279</td>\n",
       "      <td>0.113383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>0.010918</td>\n",
       "      <td>-20.859375</td>\n",
       "      <td>3.775391</td>\n",
       "      <td>-8.252844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030996</td>\n",
       "      <td>0.739916</td>\n",
       "      <td>0.068434</td>\n",
       "      <td>0.125281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>0.010803</td>\n",
       "      <td>-22.515625</td>\n",
       "      <td>3.925781</td>\n",
       "      <td>-8.397625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029464</td>\n",
       "      <td>0.756164</td>\n",
       "      <td>0.069854</td>\n",
       "      <td>0.127894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>0.010699</td>\n",
       "      <td>-22.968750</td>\n",
       "      <td>4.085938</td>\n",
       "      <td>-8.584840</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036027</td>\n",
       "      <td>0.760901</td>\n",
       "      <td>0.075109</td>\n",
       "      <td>0.136722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.011200</td>\n",
       "      <td>0.010563</td>\n",
       "      <td>-23.562500</td>\n",
       "      <td>4.320312</td>\n",
       "      <td>-8.636718</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039195</td>\n",
       "      <td>0.731256</td>\n",
       "      <td>0.084864</td>\n",
       "      <td>0.152079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.011200</td>\n",
       "      <td>0.010464</td>\n",
       "      <td>-24.750000</td>\n",
       "      <td>4.371094</td>\n",
       "      <td>-8.642120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040292</td>\n",
       "      <td>0.755608</td>\n",
       "      <td>0.084083</td>\n",
       "      <td>0.151327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.011100</td>\n",
       "      <td>0.010385</td>\n",
       "      <td>-25.593750</td>\n",
       "      <td>4.585938</td>\n",
       "      <td>-8.711390</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039257</td>\n",
       "      <td>0.741554</td>\n",
       "      <td>0.089316</td>\n",
       "      <td>0.159429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.011000</td>\n",
       "      <td>0.010316</td>\n",
       "      <td>-25.031250</td>\n",
       "      <td>4.792969</td>\n",
       "      <td>-8.727045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042549</td>\n",
       "      <td>0.751467</td>\n",
       "      <td>0.089561</td>\n",
       "      <td>0.160048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.010900</td>\n",
       "      <td>0.010264</td>\n",
       "      <td>-25.671875</td>\n",
       "      <td>4.835938</td>\n",
       "      <td>-8.812898</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040996</td>\n",
       "      <td>0.747509</td>\n",
       "      <td>0.090976</td>\n",
       "      <td>0.162210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.010800</td>\n",
       "      <td>0.010210</td>\n",
       "      <td>-26.343750</td>\n",
       "      <td>4.898438</td>\n",
       "      <td>-8.897283</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042218</td>\n",
       "      <td>0.738220</td>\n",
       "      <td>0.093620</td>\n",
       "      <td>0.166167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\judem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterSampler\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Define hyperparameter search space\n",
    "param_grid = {\n",
    "    \"learning_rate\": [1e-5],\n",
    "    \"num_train_epochs\": [5, 10],\n",
    "    \"per_device_train_batch_size\": [16, 32, 64, 128],\n",
    "    \"weight_decay\": [0.01]\n",
    "}\n",
    "\n",
    "# Generate random samples of hyperparameters\n",
    "n_trials = 5\n",
    "param_samples = list(ParameterSampler(param_grid, n_iter=n_trials, random_state=478))\n",
    "\n",
    "best_score = -np.inf\n",
    "best_params = None\n",
    "best_model = None\n",
    "\n",
    "# Iterate over sampled hyperparameters\n",
    "for i, params in enumerate(param_samples):\n",
    "    print(f\"Trial {i+1}/{n_trials} with parameters: {params}\")\n",
    "\n",
    "    # Update TrainingArguments dynamically\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results/trial_{i}\",\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=4000,\n",
    "        save_strategy=\"no\",\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        per_device_train_batch_size=params[\"per_device_train_batch_size\"],\n",
    "        per_device_eval_batch_size=16, \n",
    "        num_train_epochs=params[\"num_train_epochs\"],\n",
    "        weight_decay=params[\"weight_decay\"],\n",
    "        fp16=True  # Mixed precision for speed\n",
    "    )\n",
    "\n",
    "    # Define the Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=dev_dataset,\n",
    "        processing_class=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate on the validation set\n",
    "    metrics = trainer.evaluate(dev_dataset)\n",
    "    f1 = metrics[\"eval_optimal_threshold_f1\"]\n",
    "\n",
    "    print(f\"Trial {i+1} F1 Score: {f1}\")\n",
    "\n",
    "    # Keep track of the best model\n",
    "    if f1 > best_score:\n",
    "        best_score = f1\n",
    "        best_params = params\n",
    "        best_model = trainer\n",
    "        trainer.save_model(\"./models\")\n",
    "\n",
    "# Output the best results\n",
    "print(f\"Best F1 Score: {best_score}\")\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "# Save the best model\n",
    "best_model.save_model(\"./best-model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77f6a58-ca72-44c8-a214-1e2d991b52ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
