{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4a72bd69-a821-4a47-af97-b1ad2170fc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "NVIDIA GeForce RTX 3070 Ti:\n",
      "Allocated: 2.8 GB\n",
      "Cached:    3.5 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "# Optionally allocate a fraction of GPU memory\n",
    "torch.cuda.set_per_process_memory_fraction(0.9, 0)\n",
    "\n",
    "# Ensure GPU selected:\n",
    "print(torch.cuda.get_device_name(0) + ':')\n",
    "print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "175073ea-4071-4aaf-a332-e354846b474c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.cleaning import download_and_clean\n",
    "\n",
    "artists, tracks = download_and_clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d54a8e77-2193-4fe4-8043-282cc51d056f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         id  followers                      genres  \\\n",
      "45   0VLMVnVbJyJ4oyZs2L3Yl2       71.0          ['carnaval cadiz']   \n",
      "46   0dt23bs4w8zx154C5xdVyl       63.0          ['carnaval cadiz']   \n",
      "47   0pGhoB99qpEJEsBQxgaskQ       64.0          ['carnaval cadiz']   \n",
      "48   3HDrX2OtSuXLW5dLR85uN3       53.0          ['carnaval cadiz']   \n",
      "136  22mLrN5fkppmuUPsHx6i2G       59.0  ['classical harp', 'harp']   \n",
      "\n",
      "                             name  popularity  \n",
      "45   Las Viudas De Los Bisabuelos           6  \n",
      "46              Los De Capuchinos           5  \n",
      "47             Los “Pofesionales”           7  \n",
      "48      Los Que No Paran De Rajar           6  \n",
      "136                   Vera Dulova           3  \n"
     ]
    }
   ],
   "source": [
    "print(artists.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07ae631a-3033-429d-8484-1f0d06c826a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       id                                name  popularity  \\\n",
      "2  07A5yehtSnoedViJAZkNnc  Vivo para Quererte - Remasterizado           0   \n",
      "3  08FmqUhxtyLTn6pAh6bk45       El Prisionero - Remasterizado           0   \n",
      "4  08y9GfoqCWfOGsKdwojr5e                 Lady of the Evening           0   \n",
      "5  0BRXJHRNGQ3W4v9frnSfhu                           Ave Maria           0   \n",
      "7  0IA0Hju8CAgYfV1hwhidBH                             La Java           0   \n",
      "\n",
      "   duration_ms  explicit              artists                  id_artists  \\\n",
      "2       181640         0  ['Ignacio Corsini']  ['5LiOoJbxVSAMkBS2fUm3X2']   \n",
      "3       176907         0  ['Ignacio Corsini']  ['5LiOoJbxVSAMkBS2fUm3X2']   \n",
      "4       163080         0      ['Dick Haymes']  ['3BiJGZsyX9sJchTqcSA7Su']   \n",
      "5       178933         0      ['Dick Haymes']  ['3BiJGZsyX9sJchTqcSA7Su']   \n",
      "7       161427         0      ['Mistinguett']  ['4AxgXfD7ISvJSTObqm4aIE']   \n",
      "\n",
      "  release_date  danceability  energy  ...  loudness  mode  speechiness  \\\n",
      "2   1922-03-21         0.434  0.1770  ...   -21.180     1       0.0512   \n",
      "3   1922-03-21         0.321  0.0946  ...   -27.961     1       0.0504   \n",
      "4         1922         0.402  0.1580  ...   -16.900     0       0.0390   \n",
      "5         1922         0.227  0.2610  ...   -12.343     1       0.0382   \n",
      "7         1922         0.563  0.1840  ...   -13.757     1       0.0512   \n",
      "\n",
      "   acousticness  instrumentalness  liveness  valence    tempo  time_signature  \\\n",
      "2         0.994          0.021800    0.2120   0.4570  130.418               5   \n",
      "3         0.995          0.918000    0.1040   0.3970  169.980               3   \n",
      "4         0.989          0.130000    0.3110   0.1960  103.220               4   \n",
      "5         0.994          0.247000    0.0977   0.0539  118.891               4   \n",
      "7         0.993          0.000016    0.3250   0.6540  133.088               3   \n",
      "\n",
      "                                              genres  \n",
      "2                             [tango, vintage tango]  \n",
      "3                             [tango, vintage tango]  \n",
      "4  [swing, lounge, big band, easy listening, adul...  \n",
      "5  [swing, lounge, big band, easy listening, adul...  \n",
      "7                                  [vintage chanson]  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "print(tracks.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9364f6fa-96a5-40dd-8d47-2a78482f8567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4706"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "all_genres = set(chain.from_iterable(tracks[\"genres\"]))\n",
    "len(all_genres)\n",
    "# all_genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af999b92-4010-4fb9-b5da-6f6cc67380a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_to_index = {genre: idx for idx, genre in enumerate(sorted(all_genres))}\n",
    "# genre_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f8f25fb-1407-42a6-9989-c04e25dd56c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multihot_vector(genres, index_dict):\n",
    "    multihot = [0] * len(index_dict)\n",
    "    for genre in genres:\n",
    "        multihot[index_dict[genre]] = 1\n",
    "    return multihot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "428fca29-7f55-4b3e-9537-e2ef8afb9573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "3         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "4         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "5         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "7         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "                                ...                        \n",
       "586667    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "586668    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "586669    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "586670    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "586671    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "Name: multi_hot_genres, Length: 499064, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracks[\"multi_hot_genres\"] = tracks[\"genres\"].apply(\n",
    "    lambda genres: multihot_vector(genres, genre_to_index)\n",
    ")\n",
    "tracks[\"multi_hot_genres\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d5aedaf-66d2-4faf-aa2e-bbe8c2504aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = tracks[[\"name\", \"popularity\", \"duration_ms\", \"explicit\", \"release_date\", \"danceability\", \"energy\", \"key\", \"loudness\", \"mode\", \"speechiness\", \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\", \"tempo\", \"time_signature\"]]\n",
    "y = tracks[\"multi_hot_genres\"].tolist()\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=478)\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=478)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34f732a6-c0f2-4d27-b749-2fa9ebef55b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 399251\n",
      "Dev set size: 49906\n",
      "Test set size: 49907\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train set size: {len(X_train)}\")\n",
    "print(f\"Dev set size: {len(X_dev)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc59b6ea-b98e-42d5-8a6c-91991e03982c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of labels: 4706\n",
      "Filtered number of labels: 1529\n"
     ]
    }
   ],
   "source": [
    "# Count label frequencies in the training set\n",
    "import numpy as np\n",
    "\n",
    "label_counts = np.sum(y_train, axis=0)\n",
    "\n",
    "# Set a threshold for filtering labels\n",
    "threshold =100\n",
    "selected_labels = np.where(label_counts >= threshold)[0]\n",
    "\n",
    "# Filter labels in train, dev, and test sets\n",
    "def filter_labels(y, selected_labels):\n",
    "    return np.array([[y_sample[i] for i in selected_labels] for y_sample in y])\n",
    "\n",
    "y_train = filter_labels(y_train, selected_labels)\n",
    "y_dev = filter_labels(y_dev, selected_labels)\n",
    "y_test = filter_labels(y_test, selected_labels)\n",
    "\n",
    "# Update genre_to_index mapping\n",
    "filtered_genre_to_index = {genre: idx for idx, genre in enumerate(selected_labels)}\n",
    "index_to_genre = {v: k for k, v in filtered_genre_to_index.items()}\n",
    "\n",
    "# Step 5: Print results for verification\n",
    "print(f\"Original number of labels: {len(label_counts)}\")\n",
    "print(f\"Filtered number of labels: {len(selected_labels)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9aa7530-f23d-4471-ac80-0e9c56d18d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size after filtering: 386974\n",
      "Dev set size after filtering: 48297\n",
      "Test set size after filtering: 48376\n"
     ]
    }
   ],
   "source": [
    "# Remove rows with no positive labels\n",
    "def remove_empty_labels(X, y):\n",
    "    non_empty_indices = [i for i, labels in enumerate(y) if np.sum(labels) > 0]\n",
    "    X_filtered = X.iloc[non_empty_indices].reset_index(drop=True)\n",
    "    y_filtered = np.array([y[i] for i in non_empty_indices])\n",
    "    return X_filtered, y_filtered\n",
    "\n",
    "# Apply to train, dev, and test sets\n",
    "X_train, y_train = remove_empty_labels(X_train, y_train)\n",
    "X_dev, y_dev = remove_empty_labels(X_dev, y_dev)\n",
    "X_test, y_test = remove_empty_labels(X_test, y_test)\n",
    "\n",
    "# Verify\n",
    "print(f\"Train set size after filtering: {len(X_train)}\")\n",
    "print(f\"Dev set size after filtering: {len(X_dev)}\")\n",
    "print(f\"Test set size after filtering: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f8a9644-ecff-49a1-a508-e5519cc64df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31961ab-7c5e-4ca1-8ffc-434ae2406c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization function\n",
    "def tokenize_data(X):\n",
    "    return tokenizer(\n",
    "        list(X[\"name\"]),\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=16,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "# Dataset preparation\n",
    "class MultiLabelDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        tokenized = tokenize_data(X)\n",
    "        self.input_ids = tokenized[\"input_ids\"]\n",
    "        self.attention_mask = tokenized[\"attention_mask\"]\n",
    "        self.labels = torch.tensor(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx],\n",
    "            \"labels\": self.labels[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "29b2fa67-2d35-48f0-bfcb-d1178c908f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert splits into datasets\n",
    "train_dataset = MultiLabelDataset(X_train, y_train)\n",
    "dev_dataset = MultiLabelDataset(X_dev, y_dev)\n",
    "test_dataset = MultiLabelDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b6f708e1-5d64-4f86-beca-030d73f16309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "class MultiLabelRoBERTa(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(MultiLabelRoBERTa, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.roberta.config.hidden_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        logits = self.classifier(outputs.pooler_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            criterion = nn.BCEWithLogitsLoss()\n",
    "            loss = criterion(logits, labels.float())\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "# Metrics computation\n",
    "def compute_metrics(pred, threshold_range=(0.05, 0.55, 0.06)):\n",
    "    logits, labels = pred\n",
    "    probabilities = torch.sigmoid(torch.tensor(logits))\n",
    "\n",
    "    # Calculate logit statistics\n",
    "    prob_min = probabilities.min().item()\n",
    "    prob_max = probabilities.max().item()\n",
    "    prob_mean = probabilities.mean().item()\n",
    "\n",
    "    def prob_threshold_pred(probabilities, threshold):\n",
    "        return (probabilities > threshold).int()\n",
    "\n",
    "    def calc_metrics(labels, predictions):\n",
    "        subset_accuracy = accuracy_score(labels, predictions)\n",
    "        precision = precision_score(labels, predictions, average=\"micro\")\n",
    "        recall = recall_score(labels, predictions, average=\"micro\")\n",
    "        f1 = f1_score(labels, predictions, average=\"micro\")\n",
    "        return subset_accuracy, precision, recall, f1\n",
    "\n",
    "#Store results for thresholds\n",
    "    threshold_results = {}\n",
    "    for threshold in np.arange(*threshold_range):\n",
    "        predictions = prob_threshold_pred(probabilities, threshold).numpy()\n",
    "        subset_accuracy, precision, recall, f1 = calc_metrics(labels, predictions)\n",
    "        threshold_results[threshold] = {\n",
    "            \"accuracy\": subset_accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1\n",
    "        }\n",
    "\n",
    "    # Find optimal threshold\n",
    "    optimal_threshold = max(threshold_results, key=lambda t: threshold_results[t][\"f1\"])\n",
    "    optimal_threshold_metrics = threshold_results[optimal_threshold]\n",
    "\n",
    "    # Return detailed metrics\n",
    "    return {\n",
    "        \"prob_min\": prob_min,\n",
    "        \"prob_max\": prob_max,\n",
    "        \"prob_mean\": prob_mean,\n",
    "        \"optimal_threshold\": optimal_threshold,\n",
    "        \"optimal_threshold_accuracy\": optimal_threshold_metrics[\"accuracy\"],\n",
    "        \"optimal_threshold_precision\": optimal_threshold_metrics[\"precision\"],\n",
    "        \"optimal_threshold_recall\": optimal_threshold_metrics[\"recall\"],\n",
    "        \"optimal_threshold_f1\": optimal_threshold_metrics[\"f1\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c667123f-dd36-4247-a228-36607069bf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "num_labels = len(y_train[0])\n",
    "model = MultiLabelRoBERTa(num_labels=num_labels)\n",
    "model.roberta.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "075be418-828e-4c62-bb1a-f5bf0c8c7d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",\n",
    "#     eval_strategy=\"steps\",\n",
    "#     eval_steps=2000,\n",
    "#     save_strategy=\"steps\",\n",
    "#     save_steps=2000,\n",
    "#     learning_rate=1e-5,\n",
    "#     per_device_train_batch_size=16,\n",
    "#     # gradient_accumulation_steps=2,\n",
    "#     per_device_eval_batch_size=16,\n",
    "#     num_train_epochs=10,\n",
    "#     weight_decay=0.01,\n",
    "#     load_best_model_at_end=True,\n",
    "#     fp16=True\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "53111b9e-98ac-42ef-875b-187ddcf9e8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=dev_dataset,\n",
    "#     processing_class=tokenizer,\n",
    "#     compute_metrics=compute_metrics\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c5a70068-67c3-4ef7-a078-4f4977fb1bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the model\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f0a4f66a-b07c-46c7-bba8-f26d88542aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test set\n",
    "# trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2b1f6868-6ca4-4387-a882-ff56b5172897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_metrics(pred, top_k_range=(1, 10), threshold_range=(0.05, 0.5, 0.05)):\n",
    "#     logits, labels = pred\n",
    "\n",
    "#     # Calculate logit statistics\n",
    "#     logit_min = logits.min()\n",
    "#     logit_max = logits.max()\n",
    "#     logit_mean = logits.mean()\n",
    "\n",
    "#     def top_k_pred(logits, k):\n",
    "#         top_k_indices = np.argsort(-logits, axis=1)[:, :k]\n",
    "#         predictions = np.zeros_like(logits, dtype=int)\n",
    "#         for i, indices in enumerate(top_k_indices):\n",
    "#             predictions[i, indices] = 1\n",
    "#         return predictions\n",
    "\n",
    "#     def logit_threshold_pred(logits, threshold):\n",
    "#         return (logits > threshold).astype(int)\n",
    "\n",
    "#     def calc_metrics(labels, predictions):\n",
    "#         subset_accuracy = accuracy_score(labels, predictions)\n",
    "#         precision = precision_score(labels, predictions, average=\"micro\")\n",
    "#         recall = recall_score(labels, predictions, average=\"micro\")\n",
    "#         f1 = f1_score(labels, predictions, average=\"micro\")\n",
    "#         return subset_accuracy, precision, recall, f1\n",
    "\n",
    "#     # Store results for top-k\n",
    "#     top_k_results = {}\n",
    "#     for k in range(*top_k_range):\n",
    "#         predictions = top_k_pred(logits, k)\n",
    "#         subset_accuracy, precision, recall, f1 = calc_metrics(labels, predictions)\n",
    "#         top_k_results[k] = {\n",
    "#             \"accuracy\": subset_accuracy,\n",
    "#             \"precision\": precision,\n",
    "#             \"recall\": recall,\n",
    "#             \"f1\": f1\n",
    "#         }\n",
    "\n",
    "#     # Find optimal k\n",
    "#     optimal_k = max(top_k_results, key=lambda k: top_k_results[k][\"f1\"])\n",
    "#     optimal_k_metrics = top_k_results[optimal_k]\n",
    "\n",
    "#     # Store results for thresholds\n",
    "#     threshold_results = {}\n",
    "#     for threshold in np.arange(*threshold_range):\n",
    "#         predictions = logit_threshold_pred(logits, threshold)\n",
    "#         subset_accuracy, precision, recall, f1 = calc_metrics(labels, predictions)\n",
    "#         threshold_results[threshold] = {\n",
    "#             \"accuracy\": subset_accuracy,\n",
    "#             \"precision\": precision,\n",
    "#             \"recall\": recall,\n",
    "#             \"f1\": f1\n",
    "#         }\n",
    "\n",
    "#     # Find optimal threshold\n",
    "#     optimal_threshold = max(threshold_results, key=lambda t: threshold_results[t][\"f1\"])\n",
    "#     optimal_threshold_metrics = threshold_results[optimal_threshold]\n",
    "\n",
    "#     # Return detailed metrics\n",
    "#     return {\n",
    "#         \"logit_min\": logit_min,\n",
    "#         \"logit_max\": logit_max,\n",
    "#         \"logit_mean\": logit_mean,\n",
    "#         \"optimal_k\": optimal_k,\n",
    "#         \"optimal_k_accuracy\": optimal_k_metrics[\"accuracy\"],\n",
    "#         \"optimal_k_precision\": optimal_k_metrics[\"precision\"],\n",
    "#         \"optimal_k_recall\": optimal_k_metrics[\"recall\"],\n",
    "#         \"optimal_k_f1\": optimal_k_metrics[\"f1\"],\n",
    "#         \"optimal_threshold\": optimal_threshold,\n",
    "#         \"optimal_threshold_accuracy\": optimal_threshold_metrics[\"accuracy\"],\n",
    "#         \"optimal_threshold_precision\": optimal_threshold_metrics[\"precision\"],\n",
    "#         \"optimal_threshold_recall\": optimal_threshold_metrics[\"recall\"],\n",
    "#         \"optimal_threshold_f1\": optimal_threshold_metrics[\"f1\"]\n",
    "#     }\n",
    "\n",
    "\n",
    "\n",
    "# # Initialize the model\n",
    "# num_labels = len(y_train[0])\n",
    "# model = MultiLabelRoBERTa(num_labels=num_labels)\n",
    "# model.roberta.gradient_checkpointing_enable()\n",
    "# # Training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",\n",
    "#     eval_strategy=\"steps\",\n",
    "#     eval_steps=4000,\n",
    "#     save_strategy=\"steps\",\n",
    "#     save_steps=4000,\n",
    "#     learning_rate=1e-5,\n",
    "#     per_device_train_batch_size=32,\n",
    "#     # gradient_accumulation_steps=2,\n",
    "#     per_device_eval_batch_size=16,\n",
    "#     num_train_epochs=20,\n",
    "#     weight_decay=0.01,\n",
    "#     load_best_model_at_end=True,\n",
    "#     fp16=True\n",
    "# )\n",
    "# # Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=dev_dataset,\n",
    "#     processing_class=tokenizer,\n",
    "#     compute_metrics=compute_metrics\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5f187b57-4f24-4b56-857a-e6dcfed62751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c7962725-e355-49f4-a9f7-3335282eb031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test set\n",
    "# trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd4e3eb-e5c6-43a0-8103-b3d4c7d54ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1/5 with parameters: {'weight_decay': 0.01, 'per_device_train_batch_size': 64, 'num_train_epochs': 5, 'learning_rate': 1e-05}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b54b6353494579b99ecc93204c79a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2831, 'grad_norm': 0.2132728397846222, 'learning_rate': 9.834628741524725e-06, 'epoch': 0.08}\n",
      "{'loss': 0.1036, 'grad_norm': 0.11181221157312393, 'learning_rate': 9.669257483049447e-06, 'epoch': 0.17}\n",
      "{'loss': 0.0489, 'grad_norm': 0.05553309991955757, 'learning_rate': 9.50388622457417e-06, 'epoch': 0.25}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 50\u001b[0m\n\u001b[0;32m     40\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     41\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     42\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     46\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[0;32m     47\u001b[0m )\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Evaluate on the validation set\u001b[39;00m\n\u001b[0;32m     53\u001b[0m metrics \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate(dev_dataset)\n",
      "File \u001b[1;32mc:\\Users\\rrtor\\Desktop\\NLPFinalProject\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2164\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2162\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2165\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2169\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rrtor\\Desktop\\NLPFinalProject\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2522\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2516\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2517\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2518\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2519\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2520\u001b[0m )\n\u001b[0;32m   2521\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2522\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2525\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2526\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2527\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2528\u001b[0m ):\n\u001b[0;32m   2529\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2530\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\rrtor\\Desktop\\NLPFinalProject\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3688\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3686\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   3687\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3688\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3689\u001b[0m     \u001b[38;5;66;03m# Finally we need to normalize the loss for reporting\u001b[39;00m\n\u001b[0;32m   3690\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\rrtor\\Desktop\\NLPFinalProject\\.venv\\Lib\\site-packages\\accelerate\\accelerator.py:2244\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2243\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2244\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2245\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[0;32m   2246\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[1;32mc:\\Users\\rrtor\\Desktop\\NLPFinalProject\\.venv\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rrtor\\Desktop\\NLPFinalProject\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rrtor\\Desktop\\NLPFinalProject\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterSampler\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Define hyperparameter search space\n",
    "param_grid = {\n",
    "    \"learning_rate\": [1e-5],\n",
    "    \"num_train_epochs\": [5, 10],\n",
    "    \"per_device_train_batch_size\": [32, 64, 128, 256],\n",
    "    \"weight_decay\": [0.01]\n",
    "}\n",
    "\n",
    "# Generate random samples of hyperparameters\n",
    "n_trials = 5\n",
    "param_samples = list(ParameterSampler(param_grid, n_iter=n_trials, random_state=478))\n",
    "\n",
    "best_score = -np.inf\n",
    "best_params = None\n",
    "best_model = None\n",
    "\n",
    "# Iterate over sampled hyperparameters\n",
    "for i, params in enumerate(param_samples):\n",
    "    print(f\"Trial {i+1}/{n_trials} with parameters: {params}\")\n",
    "\n",
    "    # Update TrainingArguments dynamically\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results/trial_{i}\",\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=4000,\n",
    "        save_strategy=\"no\",\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        per_device_train_batch_size=params[\"per_device_train_batch_size\"],\n",
    "        per_device_eval_batch_size=16, \n",
    "        num_train_epochs=params[\"num_train_epochs\"],\n",
    "        weight_decay=params[\"weight_decay\"],\n",
    "        fp16=True  # Mixed precision for speed\n",
    "    )\n",
    "\n",
    "    # Define the Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=dev_dataset,\n",
    "        processing_class=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate on the validation set\n",
    "    metrics = trainer.evaluate(dev_dataset)\n",
    "    f1 = metrics[\"eval_optimal_threshold_f1\"]\n",
    "\n",
    "    print(f\"Trial {i+1} F1 Score: {f1}\")\n",
    "\n",
    "    # Keep track of the best model\n",
    "    if f1 > best_score:\n",
    "        best_score = f1\n",
    "        best_params = params\n",
    "        best_model = trainer\n",
    "        trainer.save_model(\"./models\")\n",
    "\n",
    "# Output the best results\n",
    "print(f\"Best F1 Score: {best_score}\")\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "# Save the best model\n",
    "best_model.save_model(\"./best-model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77f6a58-ca72-44c8-a214-1e2d991b52ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
